# ComfyUI-FramePack-HY/nodes/sampler.py

import torch
import math
import numpy as np
from tqdm import tqdm
import random
import traceback # Import traceback for better error logging

# å¯¼å…¥ ComfyUI ç›¸å…³
import comfy.model_management as model_management
import comfy.utils
import comfy.model_base
import comfy.latent_formats
import comfy.model_patcher

# å¯¼å…¥è‡ªå®šä¹‰é‡‡æ ·å‡½æ•° (ä½¿ç”¨ç›¸å¯¹è·¯å¾„)
from ..diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan
# å¯¼å…¥è¾…åŠ©å‡½æ•°
from ..diffusers_helper.utils import repeat_to_batch_size, crop_or_pad_yield_mask
from ..diffusers_helper.memory import move_model_to_device_with_memory_preservation, get_cuda_free_memory_gb

# VAEç¼©æ”¾å› å­ (Hunyuan Videoä½¿ç”¨0.476986)
vae_scaling_factor = 0.476986

class HyVideoModel(comfy.model_base.BaseModel):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.pipeline = {}
        self.load_device = model_management.get_torch_device()

    def __getitem__(self, k):
        return self.pipeline[k]

    def __setitem__(self, k, v):
        self.pipeline[k] = v


class HyVideoModelConfig:
    def __init__(self, dtype):
        self.unet_config = {}
        self.unet_extra_config = {}
        self.latent_format = comfy.latent_formats.HunyuanVideo
        self.latent_format.latent_channels = 16
        self.manual_cast_dtype = dtype
        self.sampling_settings = {"multiplier": 1.0}
        self.memory_usage_factor = 2.0
        self.unet_config["disable_unet_model_creation"] = True

class FramePackDiffusersSampler:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "fp_pipeline": ("FP_DIFFUSERS_PIPELINE",), # æ¥æ”¶æ¥è‡ªåŠ è½½èŠ‚ç‚¹çš„ Pipeline
                "positive": ("CONDITIONING",),
                "negative": ("CONDITIONING",),
                "steps": ("INT", {"default": 30, "min": 1, "max": 10000}),
                "cfg": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step": 0.1}), # real guidance scale
                "guidance_scale": ("FLOAT", {"default": 6.0, "min": 0.0, "max": 100.0, "step": 0.1}), # distilled guidance scale
                "seed": ("INT", {"default": 66666666, "min": 0, "max": 0xffffffffffffffff}),
                "width": ("INT", {"default": 512, "min": 256, "max": 2048, "step": 8}),
                "height": ("INT", {"default": 512, "min": 256, "max": 2048, "step": 8}),
                "gpu_memory_preservation": ("FLOAT", {"default": 6.0, "min": 0.0, "max": 100.0, "step": 0.1, 
                                                     "tooltip": "GPUå†…å­˜ä¿ç•™é‡(GB)ï¼Œè¶Šå¤§è¶Šç¨³å®šä½†é€Ÿåº¦è¶Šæ…¢"}),
                "sampler": (["unipc"],
                            {"default": "unipc", 
                             "tooltip": "é‡‡æ ·å™¨ç±»å‹ï¼Œç›®å‰ä»…æ”¯æŒunipc"}),
            },
            "optional": {
                # ä»CreateKeyframesèŠ‚ç‚¹ç›´æ¥è¿æ¥çš„è§†é¢‘å‚æ•°
                "video_length_seconds": ("video_length_seconds", {"default": None, 
                                                  "tooltip": "ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„è§†é¢‘æ—¶é•¿(ç§’)ï¼Œä¼˜å…ˆçº§æœ€é«˜"}),
                "video_fps": ("video_fps", {"default": None,  
                                     "tooltip": "ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„å¸§ç‡(fps)ï¼Œä¼˜å…ˆçº§æœ€é«˜"}),
                "window_size": ("window_size", {"default": None, 
                                       "tooltip": "ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„çª—å£å¤§å°ï¼Œä¼˜å…ˆçº§æœ€é«˜"}),
                
                "clip_vision": ("CLIP_VISION_OUTPUT", {"tooltip": "CLIP Visionçš„è¾“å‡ºï¼Œç”¨äºå›¾åƒå¼•å¯¼"}),
                "shift": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 10.0, "step": 0.1, 
                                   "tooltip": "å½±å“è¿åŠ¨å¹…åº¦ï¼Œæ•°å€¼è¶Šé«˜è¿åŠ¨è¶Šå¼º"}),
                "use_teacache": ("BOOLEAN", {"default": True, "tooltip": "ä½¿ç”¨teacacheåŠ é€Ÿé‡‡æ ·"}),
                "teacache_thresh": ("FLOAT", {"default": 0.15, "min": 0.0, "max": 1.0, "step": 0.01, 
                                             "tooltip": "teacacheç›¸å¯¹L1æŸå¤±é˜ˆå€¼"}),
                "denoise_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01, 
                                               "tooltip": "I2Væ¨¡å¼çš„å»å™ªå¼ºåº¦(å½“å‰æœªä½¿ç”¨ï¼Œæœªæ¥å¯èƒ½ç”¨äºstart_latent)"}),
                # å…³é”®å¸§ç›¸å…³å‚æ•°
                "keyframes": ("LATENT", {"tooltip": "ç”¨äºå¼•å¯¼è§†é¢‘å†…å®¹çš„å…³é”®å¸§æ½œå˜é‡é›†åˆ (é¢„æœŸä¸º[è§†è§‰ç»ˆç‚¹å¸§, è§†è§‰èµ·ç‚¹å¸§])"}),
                "keyframe_indices": ("KEYFRAME_INDICES", {"tooltip": "ä¸keyframeså¯¹åº”çš„åˆ†æ®µç´¢å¼•åˆ—è¡¨ï¼ˆå¿…é¡»å‡åºï¼Œä¾‹å¦‚: '0,N-1'ï¼‰ã€‚Nä¸ºæ€»åˆ†æ®µæ•°"}),
                "keyframe_guidance_strength": ("FLOAT", {"default": 1.5, "min": 0.1, "max": 10.0, "step": 0.1, 
                                                         "tooltip": "å…³é”®å¸§å¼•å¯¼å¼ºåº¦ã€‚æ§åˆ¶å…³é”®å¸§å¯¹è§†é¢‘çš„å½±å“ç¨‹åº¦ã€‚å€¼è¶Šé«˜ï¼Œè§†é¢‘åœ¨å…³é”®å¸§ä½ç½®è¶Šæ¥è¿‘ç›®æ ‡å›¾åƒï¼Œè¿‡æ¸¡æ•ˆæœè¶Šæ˜æ˜¾"})
            }
        }

    RETURN_TYPES = ("LATENT",)
    FUNCTION = "sample"
    CATEGORY = "FramePack"

    def sample(self, fp_pipeline, positive, negative, steps, cfg, guidance_scale, seed,
               width, height, gpu_memory_preservation, sampler, 
               total_second_length=5, fps=24, latent_window_size=9,
               video_length_seconds=None, video_fps=None, window_size=None,
               clip_vision=None, shift=0.0, use_teacache=True, 
               teacache_thresh=0.15, denoise_strength=1.0, keyframes=None, keyframe_indices="", 
               keyframe_guidance_strength=1.5):

        # ä¼˜å…ˆä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è¿æ¥çš„è§†é¢‘å‚æ•°
        if video_length_seconds is not None:
            total_second_length = video_length_seconds
            print(f"[FramePack Sampler] ä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„è§†é¢‘æ—¶é•¿: {total_second_length}ç§’")
            
        if video_fps is not None:
            fps = video_fps
            print(f"[FramePack Sampler] ä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„å¸§ç‡: {fps}fps")
            
        if window_size is not None:
            latent_window_size = window_size
            print(f"[FramePack Sampler] ä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„çª—å£å¤§å°: {latent_window_size}")
        
        print(f"[FramePack Sampler] æœ€ç»ˆè§†é¢‘å‚æ•°: æ€»æ—¶é•¿={total_second_length}ç§’, å¸§ç‡={fps}fps, çª—å£å¤§å°={latent_window_size}")
        
        # ç¡®ä¿å°ºå¯¸è¶³å¤Ÿå¤§
        if height < 256 or width < 256:
            raise ValueError(f"è¾“å…¥å°ºå¯¸å¤ªå°: {width}x{height}ï¼Œè¯·ç¡®ä¿å®½åº¦å’Œé«˜åº¦è‡³å°‘ä¸º256åƒç´ ")
        
        # ç¡®ä¿æˆ‘ä»¬æœ‰ä¸€ä¸ªåŠ è½½å¥½çš„transformer
        if "transformer" not in fp_pipeline or "dtype" not in fp_pipeline:
            raise ValueError("æ— æ•ˆçš„Pipelineå¯¹è±¡ã€‚è¯·ä½¿ç”¨Load FramePack PipelineèŠ‚ç‚¹åŠ è½½æœ‰æ•ˆçš„æ¨¡å‹ã€‚")
        
        transformer = fp_pipeline["transformer"]
        dtype = fp_pipeline["dtype"]
        
        # è®¾å¤‡å’Œæ•°æ®ç±»å‹å‡†å¤‡
        device = model_management.get_torch_device()
        offload_device = model_management.unet_offload_device()
        print(f"[FramePack Sampler] ä½¿ç”¨è®¾å¤‡: {device}, ç²¾åº¦: {dtype}")
        
        # è®¡ç®—æ½œå˜é‡å°ºå¯¸
        latent_height = height // 8
        latent_width = width // 8
        
        # è®¡ç®—è§†é¢‘å¸§æ•°å’Œåˆ†æ®µ
        num_frames_per_window = latent_window_size * 4 - 3
        total_latent_sections = (total_second_length * fps) / num_frames_per_window
        total_latent_sections = math.ceil(total_latent_sections)
        total_latent_sections = max(1, int(total_latent_sections))
        print(f"[FramePack Sampler] æ€»åˆ†æ®µæ•°: {total_latent_sections}, æ¯æ®µå¸§æ•°: {num_frames_per_window}")
        
        # å†…å­˜ç®¡ç†
        model_management.unload_all_models()
        model_management.cleanup_models()
        model_management.soft_empty_cache()
        
        # å¤„ç†æ¡ä»¶è¾“å…¥
        print("[FramePack Sampler] å¤„ç†æ¡ä»¶è¾“å…¥...")
        
        # å¤„ç†æ­£å‘æ¡ä»¶
        llama_vec = positive[0][0].to(dtype=dtype, device=device)
        clip_l_pooler = positive[0][1]["pooled_output"].to(dtype=dtype, device=device)
        
        # å¤„ç†è´Ÿå‘æ¡ä»¶
        if not math.isclose(cfg, 1.0):  # å¦‚æœéœ€è¦çœŸæ­£çš„CFG
            llama_vec_n = negative[0][0].to(dtype=dtype, device=device)
            clip_l_pooler_n = negative[0][1]["pooled_output"].to(dtype=dtype, device=device)
        else:
            # å¦‚æœCFGä¸º1.0ï¼Œåˆ›å»ºå…¨é›¶æ¡ä»¶
            llama_vec_n = torch.zeros_like(llama_vec, device=device)
            clip_l_pooler_n = torch.zeros_like(clip_l_pooler, device=device)
        
        # è£å‰ªæˆ–å¡«å……LLAMAåµŒå…¥å’Œåˆ›å»ºæ³¨æ„åŠ›æ©ç 
        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)
        llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)
        
        # å‡†å¤‡CLIPè§†è§‰ç‰¹å¾
        image_embeddings = None
        if clip_vision is not None:
            image_embeddings = clip_vision["last_hidden_state"].to(dtype=dtype, device=device)
            print(f"[FramePack Sampler] CLIPè§†è§‰ç‰¹å¾å½¢çŠ¶: {image_embeddings.shape}")
        
        # ç§»é™¤äº† start_latent çš„å¤„ç†
        batch_size = 1
        initial_latent = None # å½“å‰é€»è¾‘ä¸ä½¿ç”¨I2Vçš„ initial_latent
        
        # åˆå§‹åŒ–å†å²æ½œå˜é‡
        history_latents = torch.zeros(
            (batch_size, 16, 1 + 2 + 16, latent_height, latent_width),
            dtype=torch.float32, 
            device="cpu"
        )
        
        # å‡†å¤‡éšæœºç”Ÿæˆå™¨
        generator = torch.Generator("cpu").manual_seed(seed)
        
        # åˆ›å»ºComfyUIæ¨¡å‹å°è£…
        comfy_model = HyVideoModel(
            HyVideoModelConfig(dtype),
            model_type=comfy.model_base.ModelType.FLOW,
            device=device,
        )
        
        # åˆ›å»ºæ¨¡å‹patcher
        patcher = comfy.model_patcher.ModelPatcher(comfy_model, device, torch.device("cpu"))
        
        # åˆ›å»ºè¿›åº¦æ¡å›è°ƒå‡½æ•° - ä¿®æ”¹ä¸ºæ›´å‡†ç¡®åœ°åæ˜ å¤šåˆ†æ®µè¿›åº¦
        # è®¡ç®—æ€»æ­¥æ•°ä¸º steps * total_latent_sections
        total_steps = steps * total_latent_sections
        progress_bar = comfy.utils.ProgressBar(total_steps)
        
        # è®°å½•å½“å‰åˆ†æ®µç´¢å¼•å’Œå·²å®Œæˆçš„åˆ†æ®µæ•°
        current_section_index = 0
        
        # å®šä¹‰ä¸€ä¸ªé€‚é…k_diffusionåº“è°ƒç”¨æ ¼å¼çš„å›è°ƒå‡½æ•°
        def callback_adapter(d):
            # k_diffusionçš„callbackä¼ å…¥å‚æ•°æ˜¯ä¸€ä¸ªå­—å…¸: {'x': x, 'i': i, 'denoised': model_prev_list[-1]}
            if 'i' in d:
                step = d['i']
                # åªæ›´æ–°ä¸€æ­¥ï¼Œå› ä¸ºæ€»æ­¥æ•°å·²ç»æ˜¯è€ƒè™‘äº†æ‰€æœ‰åˆ†æ®µçš„
                progress_bar.update(1)
                
                # æ‰“å°æ›´è¯¦ç»†çš„è¿›åº¦ä¿¡æ¯
                if step % 5 == 0 or step == steps - 1:  # æ¯5æ­¥æˆ–æœ€åä¸€æ­¥æ‰“å°ä¸€æ¬¡
                    section_progress = f"{current_section_index + 1}/{total_latent_sections}"
                    overall_progress = f"{(current_section_index * steps + step + 1)}/{total_steps}"
                    print(f"[FramePack Sampler] è¿›åº¦: åˆ†æ®µ {section_progress}, æ­¥éª¤ {step + 1}/{steps}, æ€»è¿›åº¦ {overall_progress}")
            return None
            
        # ---------- æ”¹è¿›çš„æ¨¡å‹åŠ è½½ä¸å†…å­˜ç®¡ç† ----------
        print(f"[FramePack Sampler] å¼€å§‹åŠ è½½æ¨¡å‹åˆ°GPUè®¾å¤‡ï¼Œå†…å­˜ä¿ç•™é‡è®¾ç½®ä¸º {gpu_memory_preservation} GB")
        
        # æ£€æŸ¥å¯ç”¨å†…å­˜å¹¶ä¼°ç®—æ¨¡å‹å¤§å°
        try:
            current_free_memory = get_cuda_free_memory_gb(device)
            print(f"[FramePack Sampler] å½“å‰GPUå¯ç”¨å†…å­˜: {current_free_memory:.2f} GB")
            
            # å¦‚æœå†…å­˜ä¿ç•™å€¼å¤§äºå½“å‰å¯ç”¨å†…å­˜çš„80%ï¼Œå‘å‡ºè­¦å‘Šå¹¶è°ƒæ•´
            if gpu_memory_preservation > current_free_memory * 0.8:
                adjusted_preservation = current_free_memory * 0.5  # è°ƒæ•´ä¸ºå¯ç”¨å†…å­˜çš„50%
                print(f"[FramePack Sampler] è­¦å‘Š: å†…å­˜ä¿ç•™å€¼({gpu_memory_preservation}GB)è¿‡å¤§, è‡ªåŠ¨è°ƒæ•´ä¸º {adjusted_preservation:.2f}GB")
                gpu_memory_preservation = adjusted_preservation
            
            # åœ¨åŠ è½½å‰å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜
            if current_free_memory <= gpu_memory_preservation + 1.0:  # éœ€è¦è‡³å°‘ä¿ç•™å€¼+1GB
                print(f"[FramePack Sampler] è­¦å‘Š: GPUå†…å­˜ä¸è¶³! å¯ç”¨: {current_free_memory:.2f}GB, éœ€è¦: >{gpu_memory_preservation+1.0}GB")
                print(f"[FramePack Sampler] å°è¯•é™ä½é‡‡æ ·åˆ†è¾¨ç‡æˆ–é™ä½ä¿ç•™å†…å­˜å€¼")
        except Exception as e:
            print(f"[FramePack Sampler] å†…å­˜æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}")
            print("[FramePack Sampler] ç»§ç»­æ‰§è¡Œï¼Œä½†å¯èƒ½ä¸ç¨³å®š")
        
        # æ”¹è¿›çš„æ¨¡å‹åŠ è½½æ–¹æ³•
        try:
            # åˆ†é˜¶æ®µåŠ è½½æ¨¡å‹ï¼Œæ¯é˜¶æ®µæ£€æŸ¥å†…å­˜
            print(f"[FramePack Sampler] é˜¶æ®µ1: å°†æ¨¡å‹ç§»åŠ¨è‡³ {device}...")
            move_model_to_device_with_memory_preservation(
                transformer, 
                target_device=device, 
                preserved_memory_gb=gpu_memory_preservation
            )
            
            # æ£€æŸ¥åŠ è½½åçš„å†…å­˜çŠ¶æ€
            try:
                post_load_memory = get_cuda_free_memory_gb(device)
                print(f"[FramePack Sampler] æ¨¡å‹åŠ è½½åGPUå¯ç”¨å†…å­˜: {post_load_memory:.2f} GB")
                
                if post_load_memory < gpu_memory_preservation:
                    print(f"[FramePack Sampler] æ³¨æ„: åŠ è½½åå¯ç”¨å†…å­˜({post_load_memory:.2f}GB)ä½äºä¿ç•™ç›®æ ‡({gpu_memory_preservation}GB)")
                    print(f"[FramePack Sampler] å°†å°è¯•ç»§ç»­è¿è¡Œï¼Œä½†å¯èƒ½ä¼šå‡ºç°å†…å­˜ä¸è¶³é”™è¯¯")
            except Exception as e:
                print(f"[FramePack Sampler] åŠ è½½åå†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
        
        except Exception as e:
            print(f"[FramePack Sampler] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(f"[FramePack Sampler] å°è¯•ä½¿ç”¨å¤‡ç”¨åŠ è½½æ–¹æ³•...")
            
            # å¤‡ç”¨åŠ è½½æ–¹æ³• - ç›´æ¥åŠ è½½ä½†ä¸ç®¡ç†å†…å­˜
            transformer.to(device)
            print("[FramePack Sampler] ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½æ¨¡å‹å®Œæˆ")
        
        # è¿è¡Œé‡‡æ ·
        print("[FramePack Sampler] å¼€å§‹é‡‡æ ·...")
        print(f"  - å°ºå¯¸: {width}x{height}, æ€»å¸§æ•°: {total_second_length * fps}")
        print(f"  - åˆ†æ®µæ•°: {total_latent_sections}, æ¯æ®µçª—å£å¤§å°: {latent_window_size}")
        print(f"  - æ­¥æ•°: {steps}, CFG: {cfg}, Guidance Scale: {guidance_scale}")
        print(f"  - ç§å­: {seed}, ç§»ä½: {shift}")
        
        try:
            # å¤„ç†åˆ†æ®µç”Ÿæˆ
            total_generated_latent_frames = 0
            latent_paddings_list = list(reversed(range(total_latent_sections)))
            latent_paddings = latent_paddings_list.copy()
            
            # å¯¹äºé•¿è§†é¢‘ï¼Œä¼˜åŒ–åˆ†æ®µç­–ç•¥
            if total_latent_sections > 4:
                latent_paddings = [3] + [2] * (total_latent_sections - 3) + [1, 0]
                latent_paddings_list = latent_paddings.copy()
            
            # å¤„ç†å…³é”®å¸§ç´¢å¼•
            keyframe_idx_list = []
            if keyframes is not None and keyframe_indices:
                try:
                    # è§£æç´¢å¼•å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "0,5,10"
                    keyframe_idx_list = [int(idx.strip()) for idx in keyframe_indices.split(',') if idx.strip()]
                    
                    # éªŒè¯ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
                    if not keyframe_idx_list:
                        print("[FramePack Sampler] è­¦å‘Š: æä¾›äº†keyframesä½†keyframe_indicesä¸ºç©ºï¼Œå…³é”®å¸§åŠŸèƒ½å°†è¢«ç¦ç”¨")
                    else:
                        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦ä¸ºå‡åº
                        if sorted(keyframe_idx_list) != keyframe_idx_list:
                            print("[FramePack Sampler] è­¦å‘Š: keyframe_indicesä¸æ˜¯å‡åºï¼Œè‡ªåŠ¨æ’åº")
                            keyframe_idx_list = sorted(keyframe_idx_list)
                        
                        # æ£€æŸ¥å…³é”®å¸§æ•°é‡æ˜¯å¦ä¸ç´¢å¼•åŒ¹é…
                        if keyframes is not None and "samples" in keyframes:
                            kf_samples = keyframes["samples"]
                            if kf_samples.ndim == 5:  # [B, C, T, H, W]
                                num_keyframes = kf_samples.shape[2]
                                if num_keyframes != len(keyframe_idx_list):
                                    print(f"[FramePack Sampler] è­¦å‘Š: keyframe_indicesé•¿åº¦({len(keyframe_idx_list)})ä¸keyframesæ•°é‡({num_keyframes})ä¸åŒ¹é…")
                                    if num_keyframes < len(keyframe_idx_list):
                                        # æˆªæ–­ç´¢å¼•åˆ—è¡¨ä»¥åŒ¹é…å…³é”®å¸§æ•°é‡
                                        keyframe_idx_list = keyframe_idx_list[:num_keyframes]
                                        print(f"[FramePack Sampler] ç´¢å¼•åˆ—è¡¨å·²æˆªæ–­ä¸º: {keyframe_idx_list}")
                            else:
                                print(f"[FramePack Sampler] è­¦å‘Š: keyframesç»´åº¦ä¸æ­£ç¡®: {kf_samples.shape}, é¢„æœŸä¸º[B, C, T, H, W]")
                        
                        print(f"[FramePack Sampler] ä½¿ç”¨å…³é”®å¸§ç´¢å¼•: {keyframe_idx_list}, å¼•å¯¼å¼ºåº¦: {keyframe_guidance_strength}")
                except Exception as e:
                    print(f"[FramePack Sampler] è§£ækeyframe_indiceså‡ºé”™: {e}")
                    print(f"[FramePack Sampler] è¯·ç¡®ä¿æ ¼å¼æ­£ç¡®ï¼Œä¾‹å¦‚: '0,5,10'")
                    keyframe_idx_list = []  # é‡ç½®ä¸ºç©ºåˆ—è¡¨
            
            visual_end_latent = None # åœ¨å¾ªç¯å¤–åˆå§‹åŒ–
            visual_start_latent = None
            idx_visual_end = -1
            idx_visual_start = -1
            
            # åœ¨å¾ªç¯å¼€å§‹å‰å‡†å¤‡å¥½å…³é”®å¸§æ½œå˜é‡ (å¦‚æœå¯ç”¨)
            if keyframes is not None and len(keyframe_idx_list) >= 1: # ä¿®æ”¹: >= 1 å³å¯å¤„ç†
                kf_samples = keyframes["samples"] * vae_scaling_factor
                kf_samples = kf_samples.to(dtype=dtype, device="cpu")
                if kf_samples.ndim == 5:
                    if kf_samples.shape[2] >= 1: # è‡³å°‘è¦æœ‰ä¸€å¸§
                        visual_end_latent = kf_samples[:, :, 0:1, :, :].to(history_latents) # è§†è§‰ç»ˆç‚¹ (ç´¢å¼•0)
                        idx_visual_end = keyframe_idx_list[0]
                        print(f"[å…³é”®å¸§é€»è¾‘] å·²å‡†å¤‡è§†è§‰ç»ˆç‚¹(ç´¢å¼•{idx_visual_end})æ½œå˜é‡")
                        if len(keyframe_idx_list) >= 2 and kf_samples.shape[2] >= 2: # å¦‚æœæœ‰ç¬¬äºŒä¸ªå…³é”®å¸§
                             visual_start_latent = kf_samples[:, :, 1:2, :, :].to(history_latents) # è§†è§‰èµ·ç‚¹ (ç´¢å¼•1)
                             idx_visual_start = keyframe_idx_list[1]
                             print(f"[å…³é”®å¸§é€»è¾‘] å·²å‡†å¤‡è§†è§‰èµ·ç‚¹(ç´¢å¼•{idx_visual_start})æ½œå˜é‡")
                        # else: # æ³¨é‡Šæ‰ï¼Œå…è®¸åªå¤„ç†å•å¸§
                        #     print(f"[å…³é”®å¸§é€»è¾‘] è­¦å‘Š: keyframes æ•°é‡ ({kf_samples.shape[2]}) ä¸ç´¢å¼•æ•°é‡ ({len(keyframe_idx_list)}) ä¸å®Œå…¨åŒ¹é…ï¼Œä½†è‡³å°‘æœ‰ç»ˆç‚¹å¸§")
                        #     keyframe_idx_list = keyframe_idx_list[:kf_samples.shape[2]] # ç¡®ä¿ç´¢å¼•åˆ—è¡¨ä¸è¶…è¿‡å®é™…å¸§æ•°
                    else:
                         print(f"[å…³é”®å¸§é€»è¾‘] è­¦å‘Š: keyframes æ—¶é—´ç»´åº¦ä¸º0ï¼Œå…³é”®å¸§å¼•å¯¼å°†ç¦ç”¨")
                         keyframe_idx_list = []
                else:
                    print(f"[å…³é”®å¸§é€»è¾‘] è­¦å‘Š: keyframes æ ¼å¼ä¸æ­£ç¡® (shape: {kf_samples.shape})ï¼Œå…³é”®å¸§å¼•å¯¼å°†ç¦ç”¨")
                    keyframe_idx_list = [] # ç¦ç”¨å…³é”®å¸§é€»è¾‘
            else:
                print(f"[å…³é”®å¸§é€»è¾‘] æœªæä¾›å…³é”®å¸§æˆ–ç´¢å¼•åˆ—è¡¨ä¸ºç©ºï¼Œå…³é”®å¸§å¼•å¯¼å°†ç¦ç”¨")
                keyframe_idx_list = [] # ç¡®ä¿ç¦ç”¨
            
            # é€æ®µç”Ÿæˆ
            for latent_padding in latent_paddings:
                print(f"[FramePack Sampler] ç”Ÿæˆåˆ†æ®µ {latent_padding + 1}/{total_latent_sections}")
                is_last_section = latent_padding == 0
                latent_padding_size = latent_padding * latent_window_size
                
                print(f'  - å¡«å……å¤§å° = {latent_padding_size}, æ˜¯å¦æœ€ååˆ†æ®µ = {is_last_section}')
                
                # åˆ›å»ºå’Œåˆ†å‰²ç´¢å¼•
                indices = torch.arange(0, sum([1, latent_padding_size, latent_window_size, 1, 2, 16])).unsqueeze(0)
                clean_latent_indices_pre, blank_indices, latent_indices, clean_latent_indices_post, clean_latent_2x_indices, clean_latent_4x_indices = indices.split(
                    [1, latent_padding_size, latent_window_size, 1, 2, 16], dim=1
                )
                clean_latent_indices = torch.cat([clean_latent_indices_pre, clean_latent_indices_post], dim=1)
                
                # è®¡ç®—å½“å‰åˆ†æ®µç´¢å¼•
                current_section_index = total_latent_sections - latent_padding - 1
                forward_section_no = current_section_index # ä½¿ç”¨ forward_section_no è¡¨è¾¾æ›´æ¸…æ™°
                
                # --- å¼€å§‹: å…³é”®å¸§å¤„ç†é€»è¾‘ (æ¨¡ä»¿å‚è€ƒä»£ç  clean_latents_pre) ---
                default_base_latent = torch.zeros_like(history_latents[:, :, :1, :, :])
                target_latent = default_base_latent
                calculated_weight = 1.0

                # --- ä¿®æ”¹: å¤„ç†åŒå…³é”®å¸§å’Œå•å…³é”®å¸§ ---
                if len(keyframe_idx_list) == 2 and visual_end_latent is not None and visual_start_latent is not None:
                    # --- æƒ…å†µA: åŒå…³é”®å¸§ ---
                    print(f"[å…³é”®å¸§é€»è¾‘] å¤„ç†åˆ†æ®µ{forward_section_no}/{total_latent_sections-1} (åŒå…³é”®å¸§æ¨¡å¼)")
                    if forward_section_no == idx_visual_start:
                        # A.1: åˆ°è¾¾è§†è§‰èµ·ç‚¹å¸§ (æœ€åä¸€ä¸ªå¤„ç†çš„åˆ†æ®µ N-1)
                        target_latent = visual_start_latent
                        calculated_weight = keyframe_guidance_strength
                        print(f"  -> ç›®æ ‡: è§†è§‰èµ·ç‚¹, æƒé‡: {calculated_weight:.2f} (æœ€å¤§)")
                    else: # A.2: æœªåˆ°è¾¾è§†è§‰èµ·ç‚¹å¸§ (åŒ…æ‹¬è§†è§‰ç»ˆç‚¹å¸§ 0)
                        target_latent = visual_end_latent # ä¸»è¦ä½¿ç”¨è§†è§‰ç»ˆç‚¹ä½œä¸ºç›®æ ‡
                        segment_width = idx_visual_start - idx_visual_end
                        if segment_width > 0:
                            # --- ä¿æŒ+çº¿æ€§è¡°å‡ (æ¢å¤çº¿æ€§) ---
                            transition_sections = segment_width # æ€»è¿‡æ¸¡æ®µæ•°
                            hold_ratio = 0.1 # ä¿æŒæƒé‡çš„æ¯”ä¾‹ (ä¿æŒä¸Šæ¬¡è®¾ç½®çš„å€¼ï¼Œåç»­å¯è°ƒ)
                            hold_sections = math.floor(transition_sections * hold_ratio)

                            print(f"    - æ€»è¿‡æ¸¡æ®µæ•°: {transition_sections}, ä¿æŒæ®µæ•°: {hold_sections}")

                            if forward_section_no <= hold_sections:
                                # åœ¨ä¿æŒæœŸå†…ï¼Œä½¿ç”¨æœ€å¤§æƒé‡
                                calculated_weight = keyframe_guidance_strength
                                print(f"    -> ç›®æ ‡: è§†è§‰ç»ˆç‚¹, æƒé‡: {calculated_weight:.2f} (ä¿æŒæœŸ)")
                            else:
                                # åœ¨è¡°å‡æœŸå†…ï¼Œè®¡ç®—é€’å‡æƒé‡ t (çº¿æ€§)
                                decay_sections_total = transition_sections - hold_sections
                                if decay_sections_total > 0:
                                    decay_progress = (forward_section_no - hold_sections -1) / decay_sections_total
                                    decay_progress = max(0.0, min(1.0, decay_progress)) # é™åˆ¶èŒƒå›´
                                    t = 1.0 - decay_progress # t ä» 1 é€’å‡åˆ° 0 (æ¢å¤çº¿æ€§)
                                    # t = 1.0 - decay_progress**2 # æ³¨é‡Šæ‰éçº¿æ€§
                                    calculated_weight = 1.0 + (keyframe_guidance_strength - 1.0) * t
                                    # print(f"    -> ç›®æ ‡: è§†è§‰ç»ˆç‚¹, æƒé‡: {calculated_weight:.2f} (è¡°å‡æœŸ t={t:.2f}, éçº¿æ€§)") # æ³¨é‡Šæ‰
                                    print(f"    -> ç›®æ ‡: è§†è§‰ç»ˆç‚¹, æƒé‡: {calculated_weight:.2f} (è¡°å‡æœŸ t={t:.2f}, çº¿æ€§)") # æ›´æ–°æ—¥å¿—è¯´æ˜
                                else:
                                    calculated_weight = keyframe_guidance_strength # ä¿æŒæœ€å¤§æƒé‡
                                    print(f"    -> ç›®æ ‡: è§†è§‰ç»ˆç‚¹, æƒé‡: {calculated_weight:.2f} (è¡°å‡æœŸé•¿åº¦ä¸º0)")
                            # --- æƒé‡è®¡ç®—ä¿®æ”¹ç»“æŸ ---
                        else: # å¦‚æœæ€»å…±åªæœ‰1æ®µ (width=0) æˆ–ç´¢å¼•é”™è¯¯
                            calculated_weight = keyframe_guidance_strength
                            print(f"  -> ç›®æ ‡: è§†è§‰ç»ˆç‚¹, æƒé‡: {calculated_weight:.2f} (å•æ®µ/é”™è¯¯)")
                elif len(keyframe_idx_list) == 1 and visual_end_latent is not None:
                    # --- æƒ…å†µB: å•å…³é”®å¸§ (è§†è§‰ç»ˆç‚¹å¸§) ---
                     print(f"[å…³é”®å¸§é€»è¾‘] å¤„ç†åˆ†æ®µ{forward_section_no}/{total_latent_sections-1} (å•å…³é”®å¸§æ¨¡å¼)")
                     target_latent = visual_end_latent
                     calculated_weight = keyframe_guidance_strength # å…¨ç¨‹ä½¿ç”¨æœ€å¤§å¼ºåº¦
                     print(f"  -> ç›®æ ‡: è§†è§‰ç»ˆç‚¹ (å”¯ä¸€), æƒé‡: {calculated_weight:.2f} (æœ€å¤§)")
                else:
                    # --- æƒ…å†µC: æ— æœ‰æ•ˆå…³é”®å¸§ ---
                    print(f"[å…³é”®å¸§é€»è¾‘] åˆ†æ®µ{forward_section_no}: å…³é”®å¸§å¼•å¯¼æœªæ¿€æ´»æˆ–é…ç½®é”™è¯¯")
                # --- å…³é”®å¸§å¤„ç†é€»è¾‘ç»“æŸ ---
                
                # åº”ç”¨è®¡ç®—å‡ºçš„ç›®æ ‡å’Œæƒé‡
                clean_latents_pre = target_latent * calculated_weight
                
                # --- ç§»é™¤å†å²æ³¨å…¥ï¼Œä½¿ç”¨åŸå§‹å†å² --- 
                clean_latents_post, clean_latents_2x, clean_latents_4x = history_latents[:, :, :1 + 2 + 16, :, :].split([1, 2, 16], dim=2)
                # print(f"[å…³é”®å¸§é€»è¾‘] åˆ†æ®µ{forward_section_no}: ä½¿ç”¨åŸå§‹å†å² clean_latents_post") # è¿™è¡Œæ—¥å¿—å¤ªé¢‘ç¹ï¼Œå¯ä»¥æ³¨é‡Šæ‰

                # --- æ‹¼æ¥æœ€ç»ˆçš„ clean_latents --- 
                clean_latents = torch.cat([clean_latents_pre, clean_latents_post], dim=2)
                # --- ç»“æŸ: å…³é”®å¸§å¤„ç†é€»è¾‘ ---
                
                # è®¾ç½®teacache
                if hasattr(transformer, 'initialize_teacache'):
                    if use_teacache:
                        transformer.initialize_teacache(enable_teacache=True, num_steps=steps, rel_l1_thresh=teacache_thresh)
                    else:
                        transformer.initialize_teacache(enable_teacache=False)
                
                # æ£€æŸ¥å½“å‰å†…å­˜çŠ¶æ€
                try:
                    current_mem = get_cuda_free_memory_gb(device)
                    print(f"[FramePack Sampler] é‡‡æ ·å‰GPUå¯ç”¨å†…å­˜: {current_mem:.2f} GB")
                    if current_mem < 1.0:  # å¦‚æœå†…å­˜ä¸¥é‡ä¸è¶³
                        print("[FramePack Sampler] è­¦å‘Š: GPUå†…å­˜ä¸¥é‡ä¸è¶³ï¼Œå°è¯•é‡Šæ”¾éƒ¨åˆ†å†…å­˜...")
                        torch.cuda.empty_cache()
                except Exception as e:
                    print(f"[FramePack Sampler] å†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
                
                # æ‰§è¡Œé‡‡æ ·
                with torch.autocast(device_type=model_management.get_autocast_device(device), dtype=dtype, enabled=True):
                    # æ›´æ–°å½“å‰åˆ†æ®µç´¢å¼•
                    print(f"[FramePack Sampler] å¼€å§‹å¤„ç†åˆ†æ®µ {current_section_index + 1}/{total_latent_sections}")
                    
                    generated_latents = sample_hunyuan(
                        transformer=transformer,
                        sampler=sampler,  # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„é‡‡æ ·å™¨
                        initial_latent=initial_latent,  # ä½¿ç”¨åˆå§‹æ½œå˜é‡(I2Væ¨¡å¼)
                        concat_latent=None,
                        strength=denoise_strength,  # åº”ç”¨å»å™ªå¼ºåº¦
                        width=width,
                        height=height,
                        frames=num_frames_per_window,
                        real_guidance_scale=cfg,
                        distilled_guidance_scale=guidance_scale,
                        guidance_rescale=0.0,
                        shift=shift if shift > 0 else None,
                        num_inference_steps=steps,
                        batch_size=batch_size,
                        generator=generator,
                        prompt_embeds=llama_vec,
                        prompt_embeds_mask=llama_attention_mask,
                        prompt_poolers=clip_l_pooler,
                        negative_prompt_embeds=llama_vec_n,
                        negative_prompt_embeds_mask=llama_attention_mask_n,
                        negative_prompt_poolers=clip_l_pooler_n,
                        dtype=dtype,
                        device=device,
                        negative_kwargs=None,
                        callback=callback_adapter,  # ä½¿ç”¨æˆ‘ä»¬çš„é€‚é…å™¨å›è°ƒå‡½æ•°
                        # æ·»åŠ é¢å¤–å‚æ•°
                        image_embeddings=image_embeddings,
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                    )
                
                # æ£€æŸ¥é‡‡æ ·åçš„å†…å­˜çŠ¶æ€
                try:
                    post_sample_mem = get_cuda_free_memory_gb(device)
                    print(f"[FramePack Sampler] åˆ†æ®µé‡‡æ ·åGPUå¯ç”¨å†…å­˜: {post_sample_mem:.2f} GB")
                    
                    # å¦‚æœå†…å­˜ä½äºä¿ç•™å€¼çš„50%ï¼Œæ¸…ç†ç¼“å­˜
                    if post_sample_mem < gpu_memory_preservation * 0.5:
                        print("[FramePack Sampler] å†…å­˜ä¸è¶³ï¼Œæ¸…ç†ç¼“å­˜...")
                        torch.cuda.empty_cache()
                except Exception as e:
                    print(f"[FramePack Sampler] é‡‡æ ·åå†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
                
                # å¦‚æœæ˜¯æœ€åä¸€æ®µï¼Œè¿æ¥è§†è§‰ç»ˆç‚¹æ½œå˜é‡ (ä¿æŒä¿®æ­£åçš„é€»è¾‘)
                if is_last_section:
                    print("[FramePack Sampler] å¤„ç†æœ€ååˆ†æ®µ (æ—¶é—´ä¸Šçš„ç¬¬ä¸€æ®µ)ï¼Œå‡†å¤‡æ‹¼æ¥è§†è§‰ç»ˆç‚¹å¸§...")
                    if visual_end_latent is not None: 
                        concat_frame = visual_end_latent.to(dtype=generated_latents.dtype, device=generated_latents.device)
                        generated_latents = torch.cat([concat_frame, generated_latents], dim=2)
                        print(f"[FramePack Sampler] æˆåŠŸæ‹¼æ¥è§†è§‰ç»ˆç‚¹å¸§ (æ¥è‡ªç´¢å¼•0), æ–°å½¢çŠ¶: {generated_latents.shape}")
                    else:
                        print("[FramePack Sampler] å…³é”®å¸§ä¿¡æ¯ä¸è¶³ï¼Œæ‹¼æ¥é»˜è®¤é›¶æ½œå˜é‡")
                        default_first = torch.zeros_like(generated_latents[:, :, :1, :, :])
                        generated_latents = torch.cat([default_first, generated_latents], dim=2)
                
                # æ›´æ–°æ€»å¸§æ•°å’Œå†å²æ½œå˜é‡
                total_generated_latent_frames += int(generated_latents.shape[2])
                history_latents = torch.cat([generated_latents.to(history_latents), history_latents], dim=2)
                
                # è·å–å®é™…ç”Ÿæˆçš„æ½œå˜é‡
                real_history_latents = history_latents[:, :, :total_generated_latent_frames, :, :]
                
                # å¦‚æœæ˜¯æœ€åä¸€æ®µï¼Œåœæ­¢ç”Ÿæˆ
                if is_last_section:
                    break
            
            print("[FramePack Sampler] é‡‡æ ·å®Œæˆ.")
            print(f"[FramePack Sampler] è¾“å‡ºå½¢çŠ¶: {real_history_latents.shape}")
            
            # è®°å½•å®é™…è°ƒç”¨çš„é‡‡æ ·ä¿¡æ¯
            print(f"[FramePack Sampler] ğŸ“Š é‡‡æ ·ä¿¡æ¯: çª—å£å¤§å°={latent_window_size}, å¸§æ•°/çª—å£={num_frames_per_window}")
            
            # ç¡®ä¿è¿›åº¦æ¡æ˜¾ç¤ºä¸ºå®ŒæˆçŠ¶æ€
            progress_remaining = total_steps - (current_section_index + 1) * steps
            if progress_remaining > 0:
                print(f"[FramePack Sampler] æ›´æ–°è¿›åº¦æ¡åˆ°å®ŒæˆçŠ¶æ€ (å‰©ä½™ {progress_remaining} æ­¥)")
                progress_bar.update(progress_remaining)
            print("[FramePack Sampler] âœ… è¿›åº¦: 100% å®Œæˆ!")
            
            # è¿”å›ç»“æœï¼Œåº”ç”¨VAEç¼©æ”¾å› å­
            return ({"samples": real_history_latents.to(model_management.intermediate_device()) / vae_scaling_factor},)
            
        except Exception as e:
            # è¯¦ç»†è®°å½•é”™è¯¯ä¿¡æ¯
            error_message = f"[FramePack Sampler] âŒ é”™è¯¯: {str(e)}"
            print(error_message)
            print("[FramePack Sampler] ğŸ“‹ é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            
            # æ›´æ–°è¿›åº¦æ¡åˆ°é”™è¯¯çŠ¶æ€
            try:
                # è®¡ç®—å‰©ä½™æ­¥æ•°å¹¶æ›´æ–°è¿›åº¦æ¡
                if 'progress_bar' in locals() and 'current_section_index' in locals() and 'total_steps' in locals():
                    progress_so_far = min(current_section_index * steps, total_steps)
                    progress_remaining = total_steps - progress_so_far
                    if progress_remaining > 0:
                        print(f"[FramePack Sampler] ç”±äºé”™è¯¯æ›´æ–°è¿›åº¦æ¡ (å‰©ä½™ {progress_remaining} æ­¥)")
                        progress_bar.update(progress_remaining)
                print("[FramePack Sampler] âš ï¸ è¿›åº¦: ç”±äºé”™è¯¯è€Œä¸­æ–­!")
            except Exception as progress_error:
                print(f"[FramePack Sampler] æ›´æ–°è¿›åº¦æ¡å¤±è´¥: {progress_error}")
            
            # æä¾›é€šç”¨çš„å»ºè®®è§£å†³æ–¹æ¡ˆ
            print("[FramePack Sampler] ğŸ”§ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("1. æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿï¼Œå¯èƒ½éœ€è¦é™ä½åˆ†è¾¨ç‡æˆ–å‡å°‘ç”Ÿæˆçš„å¸§æ•°")
            print("2. ç¡®ä¿æ¨¡å‹æ­£ç¡®åŠ è½½")
            print("3. æ£€æŸ¥æ¡ä»¶è¾“å…¥æ˜¯å¦æœ‰æ•ˆ")
            print("4. å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯ä»¥å°è¯•é‡å¯ComfyUIæˆ–æ¸…ç†ç¼“å­˜")
            
            # åˆ›å»ºä¸€ä¸ªç©ºçš„æœ‰æ•ˆæ½œå˜é‡ä½œä¸ºè¿”å›å€¼
            try:
                print("[FramePack Sampler] åˆ›å»ºç©ºæ½œå˜é‡ä½œä¸ºé”™è¯¯æ¢å¤...")
                # åˆ›å»ºä¸€ä¸ªå°çš„ç©ºæ½œå˜é‡
                empty_latent = torch.zeros(
                    (1, 16, 1, height // 8, width // 8), 
                    dtype=torch.float32,
                    device=model_management.intermediate_device()
                )
                return ({"samples": empty_latent},)
            except Exception as fallback_error:
                print(f"[FramePack Sampler] åˆ›å»ºç©ºæ½œå˜é‡å¤±è´¥: {fallback_error}")
                # æœ€å°çš„å¯èƒ½æ½œå˜é‡
                minimal_latent = torch.zeros(
                    (1, 4, 1, 8, 8), 
                    dtype=torch.float32,
                    device=model_management.intermediate_device()
                )
                return ({"samples": minimal_latent},)
        finally:
            # ä¸»åŠ¨é‡Šæ”¾å†…å­˜
            print("[FramePack Sampler] ä¸»åŠ¨æ¸…ç†GPUå†…å­˜...")
            torch.cuda.empty_cache()
            
            # é‡Šæ”¾transformer
            try:
                print(f"[FramePack Sampler] å°†transformerå¸è½½åˆ° {offload_device}")
                transformer.to(offload_device)
            except Exception as e:
                print(f"[FramePack Sampler] å¸è½½transformeræ—¶å‡ºé”™: {e}")
            
            model_management.soft_empty_cache()


NODE_CLASS_MAPPINGS = {
    "FramePackDiffusersSampler_HY": FramePackDiffusersSampler
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "FramePackDiffusersSampler_HY": "FramePack Sampler (HY)"
} 

# å¯¼å…¥è¾…åŠ©èŠ‚ç‚¹å¹¶æ·»åŠ åˆ°æ˜ å°„ä¸­
try:
    from .keyframe_helper import NODE_CLASS_MAPPINGS as KEYFRAME_NODE_CLASS_MAPPINGS
    from .keyframe_helper import NODE_DISPLAY_NAME_MAPPINGS as KEYFRAME_NODE_DISPLAY_NAME_MAPPINGS
    
    # æ›´æ–°æ˜ å°„
    NODE_CLASS_MAPPINGS.update(KEYFRAME_NODE_CLASS_MAPPINGS)
    NODE_DISPLAY_NAME_MAPPINGS.update(KEYFRAME_NODE_DISPLAY_NAME_MAPPINGS)
except ImportError as e:
    print(f"[FramePack] è­¦å‘Š: æ— æ³•å¯¼å…¥å…³é”®å¸§è¾…åŠ©èŠ‚ç‚¹: {e}") 