# ComfyUI-FramePack-HY/nodes/sampler.py

import torch
import math
import numpy as np
from tqdm import tqdm
import random
import traceback # Import traceback for better error logging

# å¯¼å…¥ ComfyUI ç›¸å…³
import comfy.model_management as model_management
import comfy.utils
import comfy.model_base
import comfy.latent_formats
import comfy.model_patcher

# å¯¼å…¥è‡ªå®šä¹‰é‡‡æ ·å‡½æ•° (ä½¿ç”¨ç›¸å¯¹è·¯å¾„)
from ..diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan
# å¯¼å…¥è¾…åŠ©å‡½æ•°
from ..diffusers_helper.utils import repeat_to_batch_size, crop_or_pad_yield_mask
from ..diffusers_helper.memory import move_model_to_device_with_memory_preservation, get_cuda_free_memory_gb

# VAEç¼©æ”¾å› å­ (Hunyuan Videoä½¿ç”¨0.476986)
vae_scaling_factor = 0.476986

class HyVideoModel(comfy.model_base.BaseModel):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.pipeline = {}
        self.load_device = model_management.get_torch_device()

    def __getitem__(self, k):
        return self.pipeline[k]

    def __setitem__(self, k, v):
        self.pipeline[k] = v


class HyVideoModelConfig:
    def __init__(self, dtype):
        self.unet_config = {}
        self.unet_extra_config = {}
        self.latent_format = comfy.latent_formats.HunyuanVideo
        self.latent_format.latent_channels = 16
        self.manual_cast_dtype = dtype
        self.sampling_settings = {"multiplier": 1.0}
        self.memory_usage_factor = 2.0
        self.unet_config["disable_unet_model_creation"] = True

class FramePackDiffusersSampler:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "fp_pipeline": ("FP_DIFFUSERS_PIPELINE",), # æ¥æ”¶æ¥è‡ªåŠ è½½èŠ‚ç‚¹çš„ Pipeline
                "positive": ("CONDITIONING",),
                "negative": ("CONDITIONING",),
                "steps": ("INT", {"default": 30, "min": 1, "max": 10000}),
                "cfg": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step": 0.1}), # real guidance scale
                "guidance_scale": ("FLOAT", {"default": 6.0, "min": 0.0, "max": 100.0, "step": 0.1}), # distilled guidance scale
                "seed": ("INT", {"default": 66666666, "min": 0, "max": 0xffffffffffffffff}),
                "width": ("INT", {"default": 512, "min": 256, "max": 2048, "step": 8}),
                "height": ("INT", {"default": 512, "min": 256, "max": 2048, "step": 8}),
                "gpu_memory_preservation": ("FLOAT", {"default": 6.0, "min": 0.0, "max": 100.0, "step": 0.1, 
                                                     "tooltip": "GPUå†…å­˜ä¿ç•™é‡(GB)ï¼Œè¶Šå¤§è¶Šç¨³å®šä½†é€Ÿåº¦è¶Šæ…¢"}),
                "sampler": (["unipc"],
                            {"default": "unipc", 
                             "tooltip": "é‡‡æ ·å™¨ç±»å‹ï¼Œç›®å‰ä»…æ”¯æŒunipc"}),
                "start_latent_out": ("LATENT", {"tooltip": "æ¥è‡ªKeyframeèŠ‚ç‚¹çš„èµ·å§‹æ½œå˜é‡"}),
                "target_latent_out": ("LATENT", {"tooltip": "(å¯é€‰) æ¥è‡ªKeyframeèŠ‚ç‚¹çš„ç›®æ ‡æ½œå˜é‡"}),
                "target_index_out": ("INT", {"tooltip": "(å¯é€‰) ç›®æ ‡æ½œå˜é‡ç”Ÿæ•ˆçš„åˆ†æ®µç´¢å¼•"}),
            },
            "optional": {
                # ä»CreateKeyframesèŠ‚ç‚¹ç›´æ¥è¿æ¥çš„è§†é¢‘å‚æ•°
                "video_length_seconds": ("video_length_seconds", {"default": None, 
                                                  "tooltip": "ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„è§†é¢‘æ—¶é•¿(ç§’)ï¼Œä¼˜å…ˆçº§æœ€é«˜"}),
                "video_fps": ("video_fps", {"default": None,  
                                     "tooltip": "ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„å¸§ç‡(fps)ï¼Œä¼˜å…ˆçº§æœ€é«˜"}),
                "window_size": ("window_size", {"default": None, 
                                       "tooltip": "ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„çª—å£å¤§å°ï¼Œä¼˜å…ˆçº§æœ€é«˜"}),
                
                "clip_vision": ("CLIP_VISION_OUTPUT", {"tooltip": "CLIP Visionçš„è¾“å‡ºï¼Œç”¨äºå›¾åƒå¼•å¯¼"}),
                "shift": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 10.0, "step": 0.1, 
                                   "tooltip": "å½±å“è¿åŠ¨å¹…åº¦ï¼Œæ•°å€¼è¶Šé«˜è¿åŠ¨è¶Šå¼º"}),
                "use_teacache": ("BOOLEAN", {"default": True, "tooltip": "ä½¿ç”¨teacacheåŠ é€Ÿé‡‡æ ·"}),
                "teacache_thresh": ("FLOAT", {"default": 0.15, "min": 0.0, "max": 1.0, "step": 0.01, 
                                             "tooltip": "teacacheç›¸å¯¹L1æŸå¤±é˜ˆå€¼"}),
                "denoise_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01, 
                                               "tooltip": "I2Væ¨¡å¼çš„å»å™ªå¼ºåº¦(å½“å‰æœªä½¿ç”¨ï¼Œæœªæ¥å¯èƒ½ç”¨äºstart_latent)"}),
                # å…³é”®å¸§ç›¸å…³å‚æ•°
                "keyframe_guidance_strength": ("FLOAT", {"default": 1.5, "min": 0.1, "max": 10.0, "step": 0.1, 
                                                         "tooltip": "å…³é”®å¸§å¼•å¯¼å¼ºåº¦ã€‚æ§åˆ¶å…³é”®å¸§å¯¹è§†é¢‘çš„å½±å“ç¨‹åº¦ã€‚å€¼è¶Šé«˜ï¼Œè§†é¢‘åœ¨å…³é”®å¸§ä½ç½®è¶Šæ¥è¿‘ç›®æ ‡å›¾åƒï¼Œè¿‡æ¸¡æ•ˆæœè¶Šæ˜æ˜¾"})
            }
        }

    RETURN_TYPES = ("LATENT",)
    FUNCTION = "sample"
    CATEGORY = "FramePack"

    def sample(self, fp_pipeline, positive, negative, steps, cfg, guidance_scale, seed,
               width, height, gpu_memory_preservation, sampler, 
               total_second_length=5, fps=24, latent_window_size=9,
               video_length_seconds=None, video_fps=None, window_size=None,
               clip_vision=None, shift=0.0, use_teacache=True, 
               teacache_thresh=0.15, denoise_strength=1.0, 
               start_latent_out=None, target_latent_out=None, target_index_out=-1,
               keyframe_guidance_strength=1.5):

        # ä¼˜å…ˆä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è¿æ¥çš„è§†é¢‘å‚æ•°
        if video_length_seconds is not None:
            total_second_length = video_length_seconds
            print(f"[FramePack Sampler] ä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„è§†é¢‘æ—¶é•¿: {total_second_length}ç§’")
            
        if video_fps is not None:
            fps = video_fps
            print(f"[FramePack Sampler] ä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„å¸§ç‡: {fps}fps")
            
        if window_size is not None:
            latent_window_size = window_size
            print(f"[FramePack Sampler] ä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„çª—å£å¤§å°: {latent_window_size}")
        
        print(f"[FramePack Sampler] æœ€ç»ˆè§†é¢‘å‚æ•°: æ€»æ—¶é•¿={total_second_length}ç§’, å¸§ç‡={fps}fps, çª—å£å¤§å°={latent_window_size}")
        
        # ç¡®ä¿å°ºå¯¸è¶³å¤Ÿå¤§
        if height < 256 or width < 256:
            raise ValueError(f"è¾“å…¥å°ºå¯¸å¤ªå°: {width}x{height}ï¼Œè¯·ç¡®ä¿å®½åº¦å’Œé«˜åº¦è‡³å°‘ä¸º256åƒç´ ")
        
        # ç¡®ä¿æˆ‘ä»¬æœ‰ä¸€ä¸ªåŠ è½½å¥½çš„transformer
        if "transformer" not in fp_pipeline or "dtype" not in fp_pipeline:
            raise ValueError("æ— æ•ˆçš„Pipelineå¯¹è±¡ã€‚è¯·ä½¿ç”¨Load FramePack PipelineèŠ‚ç‚¹åŠ è½½æœ‰æ•ˆçš„æ¨¡å‹ã€‚")
        
        transformer = fp_pipeline["transformer"]
        dtype = fp_pipeline["dtype"]
        
        # è®¾å¤‡å’Œæ•°æ®ç±»å‹å‡†å¤‡
        device = model_management.get_torch_device()
        offload_device = model_management.unet_offload_device()
        print(f"[FramePack Sampler] ä½¿ç”¨è®¾å¤‡: {device}, ç²¾åº¦: {dtype}")
        
        # è®¡ç®—æ½œå˜é‡å°ºå¯¸
        latent_height = height // 8
        latent_width = width // 8
        
        # è®¡ç®—è§†é¢‘å¸§æ•°å’Œåˆ†æ®µ
        num_frames_per_window = latent_window_size * 4 - 3
        total_latent_sections = (total_second_length * fps) / num_frames_per_window
        total_latent_sections = math.ceil(total_latent_sections)
        total_latent_sections = max(1, int(total_latent_sections))
        print(f"[FramePack Sampler] æ€»åˆ†æ®µæ•°: {total_latent_sections}, æ¯æ®µå¸§æ•°: {num_frames_per_window}")
        
        # å†…å­˜ç®¡ç†
        model_management.unload_all_models()
        model_management.cleanup_models()
        model_management.soft_empty_cache()
        
        # å¤„ç†æ¡ä»¶è¾“å…¥
        print("[FramePack Sampler] å¤„ç†æ¡ä»¶è¾“å…¥...")
        
        # å¤„ç†æ­£å‘æ¡ä»¶
        llama_vec = positive[0][0].to(dtype=dtype, device=device)
        clip_l_pooler = positive[0][1]["pooled_output"].to(dtype=dtype, device=device)
        
        # å¤„ç†è´Ÿå‘æ¡ä»¶
        if not math.isclose(cfg, 1.0):  # å¦‚æœéœ€è¦çœŸæ­£çš„CFG
            llama_vec_n = negative[0][0].to(dtype=dtype, device=device)
            clip_l_pooler_n = negative[0][1]["pooled_output"].to(dtype=dtype, device=device)
        else:
            # å¦‚æœCFGä¸º1.0ï¼Œåˆ›å»ºå…¨é›¶æ¡ä»¶
            llama_vec_n = torch.zeros_like(llama_vec, device=device)
            clip_l_pooler_n = torch.zeros_like(clip_l_pooler, device=device)
        
        # è£å‰ªæˆ–å¡«å……LLAMAåµŒå…¥å’Œåˆ›å»ºæ³¨æ„åŠ›æ©ç 
        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)
        llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)
        
        # å‡†å¤‡CLIPè§†è§‰ç‰¹å¾
        image_embeddings = None
        if clip_vision is not None:
            image_embeddings = clip_vision["last_hidden_state"].to(dtype=dtype, device=device)
            print(f"[FramePack Sampler] CLIPè§†è§‰ç‰¹å¾å½¢çŠ¶: {image_embeddings.shape}")
        
        # ç§»é™¤äº† start_latent çš„å¤„ç†
        batch_size = 1
        initial_latent = None # å½“å‰é€»è¾‘ä¸ä½¿ç”¨I2Vçš„ initial_latent
        
        # åˆå§‹åŒ–å†å²æ½œå˜é‡ - ä¿®æ”¹ï¼šåˆå§‹åŒ–ä¸ºç©ºï¼Œå°†åœ¨å¾ªç¯ä¸­æ„å»º
        history_latents = torch.zeros(
            (batch_size, 16, 0, latent_height, latent_width), # æ—¶é—´ç»´åº¦ä»0å¼€å§‹
            dtype=torch.float32,
            device="cpu" # å­˜å‚¨åœ¨CPUä»¥èŠ‚çœæ˜¾å­˜
        )
        total_generated_latent_frames = 0 # è¿½è¸ªå·²ç”Ÿæˆçš„å¸§æ•°
        
        # å‡†å¤‡éšæœºç”Ÿæˆå™¨
        generator = torch.Generator("cpu").manual_seed(seed)
        
        # åˆ›å»ºComfyUIæ¨¡å‹å°è£…
        comfy_model = HyVideoModel(
            HyVideoModelConfig(dtype),
            model_type=comfy.model_base.ModelType.FLOW,
            device=device,
        )
        
        # åˆ›å»ºæ¨¡å‹patcher
        patcher = comfy.model_patcher.ModelPatcher(comfy_model, device, torch.device("cpu"))
        
        # åˆ›å»ºè¿›åº¦æ¡å›è°ƒå‡½æ•° - ä¿®æ”¹ä¸ºæ›´å‡†ç¡®åœ°åæ˜ å¤šåˆ†æ®µè¿›åº¦
        # è®¡ç®—æ€»æ­¥æ•°ä¸º steps * total_latent_sections
        total_steps = steps * total_latent_sections
        progress_bar = comfy.utils.ProgressBar(total_steps)
        
        # è®°å½•å½“å‰åˆ†æ®µç´¢å¼•å’Œå·²å®Œæˆçš„åˆ†æ®µæ•°
        current_section_index = 0
        
        # å®šä¹‰ä¸€ä¸ªé€‚é…k_diffusionåº“è°ƒç”¨æ ¼å¼çš„å›è°ƒå‡½æ•°
        def callback_adapter(d):
            # k_diffusionçš„callbackä¼ å…¥å‚æ•°æ˜¯ä¸€ä¸ªå­—å…¸: {'x': x, 'i': i, 'denoised': model_prev_list[-1]}
            if 'i' in d:
                step = d['i']
                # åªæ›´æ–°ä¸€æ­¥ï¼Œå› ä¸ºæ€»æ­¥æ•°å·²ç»æ˜¯è€ƒè™‘äº†æ‰€æœ‰åˆ†æ®µçš„
                progress_bar.update(1)
                
                # æ‰“å°æ›´è¯¦ç»†çš„è¿›åº¦ä¿¡æ¯
                if step % 5 == 0 or step == steps - 1:  # æ¯5æ­¥æˆ–æœ€åä¸€æ­¥æ‰“å°ä¸€æ¬¡
                    section_progress = f"{current_section_index + 1}/{total_latent_sections}"
                    overall_progress = f"{(current_section_index * steps + step + 1)}/{total_steps}"
                    print(f"[FramePack Sampler] è¿›åº¦: åˆ†æ®µ {section_progress}, æ­¥éª¤ {step + 1}/{steps}, æ€»è¿›åº¦ {overall_progress}")
            return None
            
        # ---------- æ”¹è¿›çš„æ¨¡å‹åŠ è½½ä¸å†…å­˜ç®¡ç† ----------
        print(f"[FramePack Sampler] å¼€å§‹åŠ è½½æ¨¡å‹åˆ°GPUè®¾å¤‡ï¼Œå†…å­˜ä¿ç•™é‡è®¾ç½®ä¸º {gpu_memory_preservation} GB")
        
        # æ£€æŸ¥å¯ç”¨å†…å­˜å¹¶ä¼°ç®—æ¨¡å‹å¤§å°
        try:
            current_free_memory = get_cuda_free_memory_gb(device)
            print(f"[FramePack Sampler] å½“å‰GPUå¯ç”¨å†…å­˜: {current_free_memory:.2f} GB")
            
            # å¦‚æœå†…å­˜ä¿ç•™å€¼å¤§äºå½“å‰å¯ç”¨å†…å­˜çš„80%ï¼Œå‘å‡ºè­¦å‘Šå¹¶è°ƒæ•´
            if gpu_memory_preservation > current_free_memory * 0.8:
                adjusted_preservation = current_free_memory * 0.5  # è°ƒæ•´ä¸ºå¯ç”¨å†…å­˜çš„50%
                print(f"[FramePack Sampler] è­¦å‘Š: å†…å­˜ä¿ç•™å€¼({gpu_memory_preservation}GB)è¿‡å¤§, è‡ªåŠ¨è°ƒæ•´ä¸º {adjusted_preservation:.2f}GB")
                gpu_memory_preservation = adjusted_preservation
            
            # åœ¨åŠ è½½å‰å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜
            if current_free_memory <= gpu_memory_preservation + 1.0:  # éœ€è¦è‡³å°‘ä¿ç•™å€¼+1GB
                print(f"[FramePack Sampler] è­¦å‘Š: GPUå†…å­˜ä¸è¶³! å¯ç”¨: {current_free_memory:.2f}GB, éœ€è¦: >{gpu_memory_preservation+1.0}GB")
                print(f"[FramePack Sampler] å°è¯•é™ä½é‡‡æ ·åˆ†è¾¨ç‡æˆ–é™ä½ä¿ç•™å†…å­˜å€¼")
        except Exception as e:
            print(f"[FramePack Sampler] å†…å­˜æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}")
            print("[FramePack Sampler] ç»§ç»­æ‰§è¡Œï¼Œä½†å¯èƒ½ä¸ç¨³å®š")
        
        # æ”¹è¿›çš„æ¨¡å‹åŠ è½½æ–¹æ³•
        try:
            # åˆ†é˜¶æ®µåŠ è½½æ¨¡å‹ï¼Œæ¯é˜¶æ®µæ£€æŸ¥å†…å­˜
            print(f"[FramePack Sampler] é˜¶æ®µ1: å°†æ¨¡å‹ç§»åŠ¨è‡³ {device}...")
            move_model_to_device_with_memory_preservation(
                transformer, 
                target_device=device, 
                preserved_memory_gb=gpu_memory_preservation
            )
            
            # æ£€æŸ¥åŠ è½½åçš„å†…å­˜çŠ¶æ€
            try:
                post_load_memory = get_cuda_free_memory_gb(device)
                print(f"[FramePack Sampler] æ¨¡å‹åŠ è½½åGPUå¯ç”¨å†…å­˜: {post_load_memory:.2f} GB")
                
                if post_load_memory < gpu_memory_preservation:
                    print(f"[FramePack Sampler] æ³¨æ„: åŠ è½½åå¯ç”¨å†…å­˜({post_load_memory:.2f}GB)ä½äºä¿ç•™ç›®æ ‡({gpu_memory_preservation}GB)")
                    print(f"[FramePack Sampler] å°†å°è¯•ç»§ç»­è¿è¡Œï¼Œä½†å¯èƒ½ä¼šå‡ºç°å†…å­˜ä¸è¶³é”™è¯¯")
            except Exception as e:
                print(f"[FramePack Sampler] åŠ è½½åå†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
        
        except Exception as e:
            print(f"[FramePack Sampler] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(f"[FramePack Sampler] å°è¯•ä½¿ç”¨å¤‡ç”¨åŠ è½½æ–¹æ³•...")
            
            # å¤‡ç”¨åŠ è½½æ–¹æ³• - ç›´æ¥åŠ è½½ä½†ä¸ç®¡ç†å†…å­˜
            transformer.to(device)
            print("[FramePack Sampler] ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½æ¨¡å‹å®Œæˆ")
        
        # è¿è¡Œé‡‡æ ·
        print("[FramePack Sampler] å¼€å§‹é‡‡æ ·...")
        print(f"  - å°ºå¯¸: {width}x{height}, æ€»å¸§æ•°: {total_second_length * fps}")
        print(f"  - åˆ†æ®µæ•°: {total_latent_sections}, æ¯æ®µçª—å£å¤§å°: {latent_window_size}")
        print(f"  - æ­¥æ•°: {steps}, CFG: {cfg}, Guidance Scale: {guidance_scale}")
        print(f"  - ç§å­: {seed}, ç§»ä½: {shift}")
        
        try:
            # --- ä¿®æ”¹: å¤„ç†æ–°çš„ Start-Target è¾“å…¥ --- 
            visual_start_latent = None
            visual_target_latent = None
            target_start_index = target_index_out # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ç´¢å¼•
            # === æ·»åŠ è°ƒè¯•æ‰“å° ===
            print(f"[FramePack Sampler DEBUG] Received target_index_out: {target_index_out}, Initial target_start_index: {target_start_index}")
            # ==================
            
            if start_latent_out is not None and "samples" in start_latent_out:
                # èµ·å§‹æ½œå˜é‡æ˜¯å¿…éœ€çš„ï¼Œåº”ç”¨VAEç¼©æ”¾å› å­
                vs_latent = start_latent_out["samples"]
                if vs_latent is not None and vs_latent.shape[2] > 0: # æ£€æŸ¥æ—¶é—´ç»´åº¦æ˜¯å¦æœ‰æ•ˆ
                     visual_start_latent = vs_latent * vae_scaling_factor
                     visual_start_latent = visual_start_latent.to(dtype=dtype, device="cpu") # å‡†å¤‡åœ¨CPUä¸Š
                     print(f"[å…³é”®å¸§é€»è¾‘] å·²å‡†å¤‡èµ·å§‹æ½œå˜é‡ (æ¥è‡ªstart_latent_out)ï¼Œå½¢çŠ¶: {visual_start_latent.shape}")
                else:
                    raise ValueError("è¾“å…¥çš„èµ·å§‹æ½œå˜é‡æ— æ•ˆæˆ–ä¸ºç©ºï¼")
            else:
                 raise ValueError("æœªæä¾›æœ‰æ•ˆçš„èµ·å§‹æ½œå˜é‡ (start_latent_out)ï¼")

            # å¤„ç†å¯é€‰çš„ç›®æ ‡æ½œå˜é‡
            if target_latent_out is not None and "samples" in target_latent_out and target_start_index >= 0:
                vt_latent = target_latent_out["samples"]
                if vt_latent is not None and vt_latent.shape[2] > 0: # æ£€æŸ¥æ—¶é—´ç»´åº¦æ˜¯å¦æœ‰æ•ˆ
                     visual_target_latent = vt_latent * vae_scaling_factor
                     visual_target_latent = visual_target_latent.to(dtype=dtype, device="cpu") # å‡†å¤‡åœ¨CPUä¸Š
                     print(f"[å…³é”®å¸§é€»è¾‘] å·²å‡†å¤‡ç›®æ ‡æ½œå˜é‡ (æ¥è‡ªtarget_latent_out)ï¼Œç›®æ ‡ç´¢å¼•: {target_start_index}ï¼Œå½¢çŠ¶: {visual_target_latent.shape}")
                     # ç¡®ä¿ç›®æ ‡ç´¢å¼•åœ¨èŒƒå›´å†…
                     if target_start_index >= total_latent_sections:
                         print(f"[å…³é”®å¸§é€»è¾‘] è­¦å‘Š: ç›®æ ‡ç´¢å¼• {target_start_index} è¶…å‡ºæ€»åˆ†æ®µæ•° {total_latent_sections}ï¼Œå°†è°ƒæ•´ä¸ºæœ€åä¸€ä¸ªåˆ†æ®µ {total_latent_sections - 1}")
                         target_start_index = total_latent_sections - 1
                     # ç¡®ä¿ç›®æ ‡ç´¢å¼•ä¸ä¸º0
                     if target_start_index == 0:
                         print(f"[å…³é”®å¸§é€»è¾‘] è­¦å‘Š: ç›®æ ‡ç´¢å¼•ä¸èƒ½ä¸º0ï¼Œå·²ç¦ç”¨ç›®æ ‡å¼•å¯¼ã€‚")
                         target_start_index = -1 # ç¦ç”¨ç›®æ ‡
                         visual_target_latent = None
                else:
                    print(f"[å…³é”®å¸§é€»è¾‘] æä¾›çš„ç›®æ ‡æ½œå˜é‡ä¸ºç©ºæˆ–æ— æ•ˆï¼Œå·²ç¦ç”¨ç›®æ ‡å¼•å¯¼ã€‚")
                    target_start_index = -1 # ç¦ç”¨ç›®æ ‡
                    visual_target_latent = None
            else:
                print(f"[å…³é”®å¸§é€»è¾‘] æœªæä¾›æœ‰æ•ˆçš„ç›®æ ‡æ½œå˜é‡æˆ–ç›®æ ‡ç´¢å¼•ï¼Œç¦ç”¨ç›®æ ‡å¼•å¯¼ã€‚")
                target_start_index = -1 # ç¦ç”¨ç›®æ ‡
                visual_target_latent = None
            # ------------------------------------------

            # é‡ç½®æ—§çš„å˜é‡ï¼Œä»¥é˜²æ„å¤–ä½¿ç”¨
            visual_end_latent = None 
            idx_visual_end = -1
            idx_visual_start = 0 # èµ·ç‚¹å›ºå®šä¸º0
            
            # åœ¨å¾ªç¯å¼€å§‹å‰å‡†å¤‡å¥½å…³é”®å¸§æ½œå˜é‡ - ä¿®æ”¹ï¼šé€»è¾‘å·²ç§»åˆ°ä¸Šé¢å¤„ç†è¾“å…¥çš„éƒ¨åˆ†
            # if keyframes is not None and len(keyframe_idx_list) >= 1:
            #     ...
            # else:
            #     ...

            # --- ä¿®æ”¹: ä»å‰å‘åé€æ®µç”Ÿæˆ ---
            for current_section_index in range(total_latent_sections):
                print(f"[FramePack Sampler] ç”Ÿæˆåˆ†æ®µ {current_section_index + 1}/{total_latent_sections}")
                is_first_section = current_section_index == 0
                is_last_section = current_section_index == total_latent_sections - 1

                # --- ä¿®æ”¹: å‚ç…§å‚è€ƒä»£ç å®šä¹‰ç´¢å¼•åˆ†å‰² ---
                indices = torch.arange(0, sum([1, 16, 2, 1, latent_window_size])).unsqueeze(0)
                # åˆ†å‰²æ–¹å¼: [èµ·å§‹ç›®æ ‡å¸§(1), 4xå†å²(16), 2xå†å²(2), 1xå†å²(1), å½“å‰çª—å£(window_size)]
                clean_latent_indices_start, clean_latent_4x_indices, clean_latent_2x_indices, clean_latent_1x_indices, latent_indices = indices.split(
                    [1, 16, 2, 1, latent_window_size], dim=1
                )
                # ç”¨äº sample_hunyuan çš„ clean_latents å‚æ•°çš„ç´¢å¼•
                clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices], dim=1)

                # --- ä¿®æ”¹: å…³é”®å¸§å¤„ç†é€»è¾‘ (è®¡ç®— target_latent å’Œ weight) ---
                # ä½¿ç”¨ forward_section_no è¡¨è¾¾æ›´æ¸…æ™°ï¼Œå³ current_section_index
                forward_section_no = current_section_index

                # åˆå§‹åŒ–é»˜è®¤ç›®æ ‡ (é›¶æ½œå˜é‡) å’Œæƒé‡
                # éœ€è¦åˆ›å»ºä¸€ä¸ªå½¢çŠ¶æ­£ç¡®çš„é›¶å¼ é‡ä½œä¸ºé»˜è®¤ç›®æ ‡
                # ä¿®æ”¹ï¼šä½¿ç”¨ visual_start_latent (å®ƒç°åœ¨æ˜¯å¿…éœ€çš„) çš„å½¢çŠ¶
                # if visual_start_latent is not None: 
                #     default_target_shape = visual_start_latent.shape
                # else:
                #     # åˆ›å»ºä¸€ä¸ªåŸºäºé…ç½®çš„å½¢çŠ¶ (B=1, C=16, T=1, H, W)
                #     default_target_shape = (batch_size, 16, 1, latent_height, latent_width)
                default_target_shape = visual_start_latent.shape # ç›´æ¥ä½¿ç”¨start_latentçš„å½¢çŠ¶

                # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡å’Œç±»å‹ä¸Šåˆ›å»º
                default_target_latent = torch.zeros(default_target_shape, dtype=dtype, device=device)

                target_latent = default_target_latent # é»˜è®¤ä¸ºé›¶
                calculated_weight = 1.0 # é»˜è®¤æƒé‡

                # --- å…³é”®å¸§å¼•å¯¼é€»è¾‘ (ä¸ä¹‹å‰ç±»ä¼¼ï¼ŒåŸºäº forward_section_no) ---
                # --- ä¿®æ”¹ï¼šå®ç°æ–°çš„ Start -> Target å¼•å¯¼é€»è¾‘ ---
                print(f"[å…³é”®å¸§é€»è¾‘] å¤„ç†åˆ†æ®µ {forward_section_no}/{total_latent_sections - 1} (Start-Targetæ¨¡å¼)")

                # --- ä¿®æ”¹ï¼šæŒç»­å¼•å¯¼ï¼Œä¸­é€”åˆ‡æ¢ --- 
                if visual_target_latent is not None and target_start_index > 0 and forward_section_no >= target_start_index:
                    # è¾¾åˆ°æˆ–è¶…è¿‡ç›®æ ‡ç´¢å¼•ï¼Œå¼ºå¼•å¯¼è‡³ç›®æ ‡
                    target_latent = visual_target_latent.to(device=device, dtype=dtype)
                    calculated_weight = keyframe_guidance_strength
                    print(f"  -> åˆ†æ®µè¾¾åˆ°/è¶…è¿‡ç›®æ ‡ç´¢å¼•({target_start_index})ï¼Œå¼ºå¼•å¯¼è‡³ç›®æ ‡ï¼Œæƒé‡: {calculated_weight:.2f}")
                else:
                    # åœ¨ç›®æ ‡ç´¢å¼•ä¹‹å‰ (åŒ…æ‹¬ç´¢å¼•0) æˆ–æ— ç›®æ ‡æ—¶ï¼Œå¼ºå¼•å¯¼è‡³èµ·ç‚¹
                    target_latent = visual_start_latent.to(device=device, dtype=dtype)
                    calculated_weight = keyframe_guidance_strength
                    print(f"  -> å¼•å¯¼è‡³èµ·ç‚¹ï¼Œæƒé‡: {calculated_weight:.2f}")
                
                # --- ä¿®æ”¹: å‡†å¤‡ clean_latents (å‚ç…§å‚è€ƒä»£ç ) --- 
                # éœ€è¦ä» history_latents æœ«å°¾æå–å†å²ä¿¡æ¯
                # å¤„ç† history_latents ä¸è¶³çš„æƒ…å†µ (ç‰¹åˆ«æ˜¯ç¬¬ä¸€å¸§)

                # éœ€è¦çš„å†å²é•¿åº¦: 1 (1x) + 2 (2x) + 16 (4x) = 19
                required_history_len = 1 + 2 + 16
                current_history_len = history_latents.shape[2]

                # åˆ›å»ºé›¶å€¼å ä½ç¬¦ï¼Œç”¨äºå¡«å……ä¸è¶³çš„å†å²
                zero_latent_1x = torch.zeros((batch_size, 16, 1, latent_height, latent_width), dtype=dtype, device=device)
                zero_latent_2x = torch.zeros((batch_size, 16, 2, latent_height, latent_width), dtype=dtype, device=device)
                zero_latent_4x = torch.zeros((batch_size, 16, 16, latent_height, latent_width), dtype=dtype, device=device)

                if current_history_len == 0:
                    # ç¬¬ä¸€å¸§ï¼Œå®Œå…¨æ²¡æœ‰å†å²
                    clean_latents_1x = zero_latent_1x
                    clean_latents_2x = zero_latent_2x
                    clean_latents_4x = zero_latent_4x
                    print(f"[FramePack Sampler] ç¬¬ä¸€ä¸ªåˆ†æ®µï¼Œä½¿ç”¨é›¶å†å²")
                else:
                    # ä»å†å²æœ«å°¾æå–ï¼Œä¸è¶³éƒ¨åˆ†ç”¨é›¶å¡«å……
                    print(f"[FramePack Sampler] ä»å†å²æ½œå˜é‡(é•¿åº¦ {current_history_len})æœ«å°¾æå–ä¸Šä¸‹æ–‡")
                    # æå–å¯ç”¨å†å²
                    available_history = history_latents[:, :, -min(current_history_len, required_history_len):, :, :].to(device=device, dtype=dtype)
                    available_len = available_history.shape[2]

                    # åˆ†é…ç»™ 1x, 2x, 4x (ä»åå¾€å‰)
                    len_1x = min(available_len, 1)
                    start_idx_1x = available_len - len_1x
                    actual_1x = available_history[:, :, start_idx_1x : start_idx_1x + len_1x, :, :]
                    clean_latents_1x = torch.cat([zero_latent_1x[:, :, :(1 - len_1x), :, :], actual_1x], dim=2) if len_1x < 1 else actual_1x

                    available_len -= len_1x
                    len_2x = min(available_len, 2)
                    start_idx_2x = available_len - len_2x
                    actual_2x = available_history[:, :, start_idx_2x : start_idx_2x + len_2x, :, :]
                    clean_latents_2x = torch.cat([zero_latent_2x[:, :, :(2 - len_2x), :, :], actual_2x], dim=2) if len_2x < 2 else actual_2x

                    available_len -= len_2x
                    len_4x = min(available_len, 16)
                    start_idx_4x = available_len - len_4x
                    actual_4x = available_history[:, :, start_idx_4x : start_idx_4x + len_4x, :, :]
                    clean_latents_4x = torch.cat([zero_latent_4x[:, :, :(16 - len_4x), :, :], actual_4x], dim=2) if len_4x < 16 else actual_4x

                    print(f"  - æå–é•¿åº¦: 1x={clean_latents_1x.shape[2]}, 2x={clean_latents_2x.shape[2]}, 4x={clean_latents_4x.shape[2]}")


                # ç»„åˆæœ€ç»ˆçš„ clean_latents
                # --- ä¿®æ”¹ï¼šå®ç°ä¸‰é˜¶æ®µå¼•å¯¼é€»è¾‘ ---
                is_guiding_target = visual_target_latent is not None and target_start_index > 0
                
                if is_guiding_target and forward_section_no == target_start_index:
                    # ç¬¬äºŒé˜¶æ®µï¼šè¿‡æ¸¡åˆ°ç›®æ ‡ (ä»…åœ¨æ­¤åˆ†æ®µä½¿ç”¨æ··åˆ)
                    alpha = 0.5 # æ··åˆæ¯”ä¾‹
                    weighted_target = target_latent * calculated_weight
                    # å½¢çŠ¶æ£€æŸ¥
                    if clean_latents_1x.shape != weighted_target.shape:
                         print(f"[FramePack Sampler] è­¦å‘Šï¼šæ··åˆå¼•å¯¼æ—¶å½¢çŠ¶ä¸åŒ¹é…ï¼ History: {clean_latents_1x.shape}, Target: {weighted_target.shape}. è·³è¿‡æ··åˆï¼Œä½¿ç”¨ [target, target]")
                         mixed_latent = weighted_target # å›é€€åˆ°ä¹‹å‰çš„å¼ºå¼•å¯¼
                    else:
                         mixed_latent = (1 - alpha) * clean_latents_1x + alpha * weighted_target
                         print(f"[FramePack Sampler] è¿‡æ¸¡è‡³ç›®æ ‡ (æ··åˆ alpha={alpha})")
                    
                    clean_latents = torch.cat([weighted_target, mixed_latent], dim=2)
                    print(f"[FramePack Sampler] ç»„åˆ clean_latents: [target, mixed(history, target)]")

                elif is_guiding_target and forward_section_no > target_start_index:
                     # ç¬¬ä¸‰é˜¶æ®µï¼šä»ç›®æ ‡çŠ¶æ€å¼€å§‹æ¼”å˜ (ä½¿ç”¨åå‘å†å²çš„æ··åˆä¸Šä¸‹æ–‡)
                     alpha_evo = 0.1 # æ¼”å˜é˜¶æ®µæ··åˆæ¯”ä¾‹ï¼Œæ›´åå‘å†å²
                     weighted_target = target_latent * calculated_weight # ç¡®ä¿ target_latent æ˜¯ç›®æ ‡æ½œå˜é‡
                     # å½¢çŠ¶æ£€æŸ¥
                     if clean_latents_1x.shape != weighted_target.shape:
                         print(f"[FramePack Sampler] è­¦å‘Šï¼šæ¼”å˜é˜¶æ®µæ··åˆæ—¶å½¢çŠ¶ä¸åŒ¹é…ï¼ History: {clean_latents_1x.shape}, Target: {weighted_target.shape}. ä½¿ç”¨ [target, history_1x]")
                         mixed_latent_evo = clean_latents_1x # å›é€€åˆ°åªä½¿ç”¨å†å²
                     else:
                         mixed_latent_evo = (1 - alpha_evo) * clean_latents_1x + alpha_evo * weighted_target
                         print(f"[FramePack Sampler] ä»ç›®æ ‡æ¼”å˜ (æ··åˆä¸Šä¸‹æ–‡ alpha={alpha_evo})")
                    
                     clean_latents = torch.cat([weighted_target, mixed_latent_evo], dim=2)
                     print(f"[FramePack Sampler] ç»„åˆ clean_latents: [target, mixed_evo(history, target)]")

                else: # å¯¹åº” forward_section_no < target_start_index æˆ–æ— ç›®æ ‡çš„æƒ…å†µ
                    # ç¬¬ä¸€é˜¶æ®µï¼šå¼•å¯¼è‡³èµ·ç‚¹ (ä½¿ç”¨çœŸå®å†å²)
                    # æ³¨æ„ï¼šæ­¤æ—¶ target_latent å®é™…ä¸Šæ˜¯ visual_start_latent
                    weighted_start = target_latent * calculated_weight
                    clean_latents = torch.cat([weighted_start, clean_latents_1x], dim=2)
                    print(f"[FramePack Sampler] å¼•å¯¼è‡³èµ·ç‚¹ï¼Œç»„åˆ clean_latents: [start, history_1x]")
                # -----------------------------------
                
                # ç»“æ„: [ç›®æ ‡å…³é”®å¸§(åŠ æƒ), 1xå†å²]
                # clean_latents = torch.cat([target_latent * calculated_weight, clean_latents_1x], dim=2)
                print(f"[FramePack Sampler] å‡†å¤‡å¥½çš„ clean_latents å½¢çŠ¶: {clean_latents.shape}")
                print(f"  - clean_latents_1x å½¢çŠ¶: {clean_latents_1x.shape}")
                print(f"  - clean_latents_2x å½¢çŠ¶: {clean_latents_2x.shape}")
                print(f"  - clean_latents_4x å½¢çŠ¶: {clean_latents_4x.shape}")

                # è®¾ç½®teacache
                if hasattr(transformer, 'initialize_teacache'):
                    if use_teacache:
                        transformer.initialize_teacache(enable_teacache=True, num_steps=steps, rel_l1_thresh=teacache_thresh)
                    else:
                        transformer.initialize_teacache(enable_teacache=False)
                
                # æ£€æŸ¥å½“å‰å†…å­˜çŠ¶æ€
                try:
                    current_mem = get_cuda_free_memory_gb(device)
                    print(f"[FramePack Sampler] é‡‡æ ·å‰GPUå¯ç”¨å†…å­˜: {current_mem:.2f} GB")
                    if current_mem < 1.0:  # å¦‚æœå†…å­˜ä¸¥é‡ä¸è¶³
                        print("[FramePack Sampler] è­¦å‘Š: GPUå†…å­˜ä¸¥é‡ä¸è¶³ï¼Œå°è¯•é‡Šæ”¾éƒ¨åˆ†å†…å­˜...")
                        torch.cuda.empty_cache()
                except Exception as e:
                    print(f"[FramePack Sampler] å†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
                
                # æ‰§è¡Œé‡‡æ ·
                with torch.autocast(device_type=model_management.get_autocast_device(device), dtype=dtype, enabled=True):
                    # æ›´æ–°å½“å‰åˆ†æ®µç´¢å¼• (è™½ç„¶å¾ªç¯å˜é‡å°±æ˜¯å®ƒï¼Œä½†ä¸ºäº†å›è°ƒå‡½æ•°æ¸…æ™°)
                    # print(f"[FramePack Sampler] å¼€å§‹å¤„ç†åˆ†æ®µ {current_section_index + 1}/{total_latent_sections}") # æ—¥å¿—ä½ç½®è°ƒæ•´

                    generated_latents = sample_hunyuan(
                        transformer=transformer,
                        sampler=sampler,  # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„é‡‡æ ·å™¨
                        initial_latent=initial_latent,  # è®¾ä¸º Noneï¼Œç”± clean_latents æ§åˆ¶
                        concat_latent=None, # -> å‚è€ƒä»£ç æœªä½¿ç”¨ï¼Œä¿æŒNone
                        strength=denoise_strength,  # åº”ç”¨å»å™ªå¼ºåº¦ (I2Væ—¶å¯èƒ½æœ‰ç”¨)
                        width=width,
                        height=height,
                        frames=num_frames_per_window,
                        real_guidance_scale=cfg,
                        distilled_guidance_scale=guidance_scale,
                        guidance_rescale=0.0,
                        shift=shift if shift > 0 else None,
                        num_inference_steps=steps,
                        batch_size=batch_size,
                        generator=generator,
                        prompt_embeds=llama_vec,
                        prompt_embeds_mask=llama_attention_mask,
                        prompt_poolers=clip_l_pooler,
                        negative_prompt_embeds=llama_vec_n,
                        negative_prompt_embeds_mask=llama_attention_mask_n,
                        negative_prompt_poolers=clip_l_pooler_n,
                        dtype=dtype,
                        device=device,
                        negative_kwargs=None, # -> å‚è€ƒä»£ç æœªä½¿ç”¨ï¼Œä¿æŒNone
                        callback=callback_adapter,  # ä½¿ç”¨æˆ‘ä»¬çš„é€‚é…å™¨å›è°ƒå‡½æ•°
                        # æ·»åŠ é¢å¤–å‚æ•° - ä½¿ç”¨æ–°çš„ç´¢å¼•å’Œclean_latents
                        image_embeddings=image_embeddings, # CLIP Vision ç‰¹å¾
                        latent_indices=latent_indices, # å½“å‰çª—å£ç´¢å¼• [T]
                        clean_latents=clean_latents, # [ç›®æ ‡å¸§(åŠ æƒ), 1xå†å²] [B,C,1+1,H,W]
                        clean_latent_indices=clean_latent_indices, # å¯¹åº” clean_latents çš„ç´¢å¼• [1+1]
                        clean_latents_2x=clean_latents_2x, # [2xå†å²] [B,C,2,H,W]
                        clean_latent_2x_indices=clean_latent_2x_indices, # [2]
                        clean_latents_4x=clean_latents_4x, # [4xå†å²] [B,C,16,H,W]
                        clean_latent_4x_indices=clean_latent_4x_indices, # [16]
                    )
                
                # æ£€æŸ¥é‡‡æ ·åçš„å†…å­˜çŠ¶æ€
                try:
                    post_sample_mem = get_cuda_free_memory_gb(device)
                    print(f"[FramePack Sampler] åˆ†æ®µé‡‡æ ·åGPUå¯ç”¨å†…å­˜: {post_sample_mem:.2f} GB")
                    
                    # å¦‚æœå†…å­˜ä½äºä¿ç•™å€¼çš„50%ï¼Œæ¸…ç†ç¼“å­˜
                    if post_sample_mem < gpu_memory_preservation * 0.5:
                        print("[FramePack Sampler] å†…å­˜ä¸è¶³ï¼Œæ¸…ç†ç¼“å­˜...")
                        torch.cuda.empty_cache()
                except Exception as e:
                    print(f"[FramePack Sampler] é‡‡æ ·åå†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
                
                # --- ä¿®æ”¹: æ›´æ–°å†å²æ½œå˜é‡å’Œæ€»å¸§æ•° ---
                # å°†ç”Ÿæˆçš„æ½œå˜é‡ç§»åˆ°CPUå¹¶æ‹¼æ¥åˆ° history_latents
                current_generated_frames = generated_latents.shape[2]
                history_latents = torch.cat([history_latents, generated_latents.to(history_latents)], dim=2)
                total_generated_latent_frames += current_generated_frames
                print(f"[FramePack Sampler] åˆ†æ®µ {current_section_index + 1} ç”Ÿæˆäº† {current_generated_frames} å¸§ï¼Œæ€»å¸§æ•°: {total_generated_latent_frames}")
                print(f"[FramePack Sampler] æ›´æ–°å history_latents å½¢çŠ¶: {history_latents.shape}")

                # ç§»é™¤æ—§çš„åå‘é€»è¾‘å’Œæœ€ååˆ†æ®µçš„ç‰¹æ®Šå¤„ç†
                # if is_last_section:
                #     # ç§»é™¤é™æ€å¸§æ·»åŠ é€»è¾‘ï¼Œé˜²æ­¢è§†é¢‘å¼€å¤´å‡ºç°é™æ€ç”»é¢
                #     print("[FramePack Sampler] å¤„ç†æœ€ååˆ†æ®µ (æ—¶é—´ä¸Šçš„ç¬¬ä¸€æ®µ)")
                #     # ä¸å†é¢å¤–æ·»åŠ é™æ€å¸§ï¼Œä¿æŒåŠ¨æ€æ•ˆæœ
                #     print(f"[FramePack Sampler] ä¿æŒåŠ¨æ€å¼€å§‹ï¼Œå½¢çŠ¶: {generated_latents.shape}")

                # æ›´æ–°æ€»å¸§æ•°å’Œå†å²æ½œå˜é‡ (å·²ç§»åˆ°ä¸Šé¢)
                # total_generated_latent_frames += int(generated_latents.shape[2])
                # history_latents = torch.cat([generated_latents.to(history_latents), history_latents], dim=2)

                # è·å–å®é™…ç”Ÿæˆçš„æ½œå˜é‡ (å·²ç§»åˆ°å¾ªç¯å¤–)
                # real_history_latents = history_latents[:, :, :total_generated_latent_frames, :, :]

                # å¦‚æœæ˜¯æœ€åä¸€æ®µï¼Œåœæ­¢ç”Ÿæˆ (ç°åœ¨ç”± for å¾ªç¯æ§åˆ¶)
                # if is_last_section:
                #     break

            # --- ä¿®æ”¹: å¾ªç¯ç»“æŸåå¤„ç†æœ€ç»ˆç»“æœ ---
            print("[FramePack Sampler] æ‰€æœ‰åˆ†æ®µé‡‡æ ·å®Œæˆ.")

            # æ£€æŸ¥æœ€ç»ˆç”Ÿæˆçš„å¸§æ•°æ˜¯å¦åˆç†
            expected_total_frames = total_second_length * fps
            # æ³¨æ„: Hunyuan Video çš„ latent_window_size å’Œ num_frames_per_window è®¡ç®—å¯èƒ½å¯¼è‡´å®é™…å¸§æ•°ç•¥æœ‰å·®å¼‚
            print(f"[FramePack Sampler] é¢„æœŸæ€»å¸§æ•°: {expected_total_frames:.1f}, å®é™…ç”Ÿæˆæ½œå˜é‡å¸§æ•°: {total_generated_latent_frames}")
            print(f"[FramePack Sampler] æœ€ç»ˆè¾“å‡º history_latents å½¢çŠ¶: {history_latents.shape}")

            # è®°å½•å®é™…è°ƒç”¨çš„é‡‡æ ·ä¿¡æ¯
            print(f"[FramePack Sampler] ğŸ“Š é‡‡æ ·ä¿¡æ¯: çª—å£å¤§å°={latent_window_size}, å¸§æ•°/çª—å£={num_frames_per_window}")

            # ç¡®ä¿è¿›åº¦æ¡æ˜¾ç¤ºä¸ºå®ŒæˆçŠ¶æ€
            # è®¡ç®—å·²å®Œæˆçš„æ€»æ­¥æ•°
            completed_steps = total_latent_sections * steps
            progress_remaining = total_steps - completed_steps # total_steps = steps * total_latent_sections
            if progress_remaining != 0: # ç†è®ºä¸Šåº”è¯¥ä¸º0ï¼Œé™¤éè®¡ç®—æœ‰è¯¯æˆ–æå‰ä¸­æ–­
                 print(f"[FramePack Sampler] è­¦å‘Š: è¿›åº¦è®¡ç®—å¯èƒ½å­˜åœ¨åå·®ï¼Œå‰©ä½™ {progress_remaining} æ­¥")
                 # å¼ºåˆ¶æ›´æ–°åˆ°100%
                 progress_bar.update(progress_remaining)

            print("[FramePack Sampler] âœ… è¿›åº¦: 100% å®Œæˆ!")

            # è¿”å›ç»“æœï¼Œåº”ç”¨VAEç¼©æ”¾å› å­
            # history_latents å·²ç»åœ¨CPUä¸Š
            final_latents = history_latents.to(model_management.intermediate_device()) / vae_scaling_factor
            print(f"[FramePack Sampler] è¿”å›æœ€ç»ˆæ½œå˜é‡ï¼Œå½¢çŠ¶: {final_latents.shape}")
            return ({"samples": final_latents},)

        except Exception as e:
            # è¯¦ç»†è®°å½•é”™è¯¯ä¿¡æ¯
            error_message = f"[FramePack Sampler] âŒ é”™è¯¯: {str(e)}"
            print(error_message)
            print("[FramePack Sampler] ğŸ“‹ é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            
            # æ›´æ–°è¿›åº¦æ¡åˆ°é”™è¯¯çŠ¶æ€
            try:
                # è®¡ç®—å‰©ä½™æ­¥æ•°å¹¶æ›´æ–°è¿›åº¦æ¡
                if 'progress_bar' in locals() and 'current_section_index' in locals() and 'total_steps' in locals():
                    progress_so_far = min(current_section_index * steps, total_steps)
                    progress_remaining = total_steps - progress_so_far
                    if progress_remaining > 0:
                        print(f"[FramePack Sampler] ç”±äºé”™è¯¯æ›´æ–°è¿›åº¦æ¡ (å‰©ä½™ {progress_remaining} æ­¥)")
                        progress_bar.update(progress_remaining)
                print("[FramePack Sampler] âš ï¸ è¿›åº¦: ç”±äºé”™è¯¯è€Œä¸­æ–­!")
            except Exception as progress_error:
                print(f"[FramePack Sampler] æ›´æ–°è¿›åº¦æ¡å¤±è´¥: {progress_error}")
            
            # æä¾›é€šç”¨çš„å»ºè®®è§£å†³æ–¹æ¡ˆ
            print("[FramePack Sampler] ğŸ”§ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("1. æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿï¼Œå¯èƒ½éœ€è¦é™ä½åˆ†è¾¨ç‡æˆ–å‡å°‘ç”Ÿæˆçš„å¸§æ•°")
            print("2. ç¡®ä¿æ¨¡å‹æ­£ç¡®åŠ è½½")
            print("3. æ£€æŸ¥æ¡ä»¶è¾“å…¥æ˜¯å¦æœ‰æ•ˆ")
            print("4. å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯ä»¥å°è¯•é‡å¯ComfyUIæˆ–æ¸…ç†ç¼“å­˜")
            
            # åˆ›å»ºä¸€ä¸ªç©ºçš„æœ‰æ•ˆæ½œå˜é‡ä½œä¸ºè¿”å›å€¼
            try:
                print("[FramePack Sampler] åˆ›å»ºç©ºæ½œå˜é‡ä½œä¸ºé”™è¯¯æ¢å¤...")
                # åˆ›å»ºä¸€ä¸ªå°çš„ç©ºæ½œå˜é‡
                empty_latent = torch.zeros(
                    (1, 16, 1, height // 8, width // 8), # ä¿æŒé€šé“æ•°16
                    dtype=torch.float32,
                    device=model_management.intermediate_device()
                )
                return ({"samples": empty_latent},)
            except Exception as fallback_error:
                print(f"[FramePack Sampler] åˆ›å»ºç©ºæ½œå˜é‡å¤±è´¥: {fallback_error}")
                # æœ€å°çš„å¯èƒ½æ½œå˜é‡
                minimal_latent = torch.zeros(
                    (1, 16, 1, 8, 8), # ä¿æŒé€šé“æ•°16
                    dtype=torch.float32,
                    device=model_management.intermediate_device()
                )
                return ({"samples": minimal_latent},)
        finally:
            # ä¸»åŠ¨é‡Šæ”¾å†…å­˜
            print("[FramePack Sampler] ä¸»åŠ¨æ¸…ç†GPUå†…å­˜...")
            torch.cuda.empty_cache()
            
            # é‡Šæ”¾transformer
            try:
                print(f"[FramePack Sampler] å°†transformerå¸è½½åˆ° {offload_device}")
                transformer.to(offload_device)
            except Exception as e:
                print(f"[FramePack Sampler] å¸è½½transformeræ—¶å‡ºé”™: {e}")
            
            model_management.soft_empty_cache()


NODE_CLASS_MAPPINGS = {
    "FramePackDiffusersSampler_HY": FramePackDiffusersSampler
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "FramePackDiffusersSampler_HY": "FramePack Sampler (HY)"
} 

# å¯¼å…¥è¾…åŠ©èŠ‚ç‚¹å¹¶æ·»åŠ åˆ°æ˜ å°„ä¸­
try:
    from .keyframe_helper import NODE_CLASS_MAPPINGS as KEYFRAME_NODE_CLASS_MAPPINGS
    from .keyframe_helper import NODE_DISPLAY_NAME_MAPPINGS as KEYFRAME_NODE_DISPLAY_NAME_MAPPINGS
    
    # æ›´æ–°æ˜ å°„
    NODE_CLASS_MAPPINGS.update(KEYFRAME_NODE_CLASS_MAPPINGS)
    NODE_DISPLAY_NAME_MAPPINGS.update(KEYFRAME_NODE_DISPLAY_NAME_MAPPINGS)
except ImportError as e:
    print(f"[FramePack] è­¦å‘Š: æ— æ³•å¯¼å…¥å…³é”®å¸§è¾…åŠ©èŠ‚ç‚¹: {e}") 