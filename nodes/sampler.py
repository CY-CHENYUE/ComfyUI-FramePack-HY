# ComfyUI-FramePack-HY/nodes/sampler.py

import torch
import math
import numpy as np
from tqdm import tqdm
import random
import traceback # Import traceback for better error logging

# å¯¼å…¥ ComfyUI ç›¸å…³
import comfy.model_management as model_management
import comfy.utils
import comfy.model_base
import comfy.latent_formats
import comfy.model_patcher

# å¯¼å…¥è‡ªå®šä¹‰é‡‡æ ·å‡½æ•° (ä½¿ç”¨ç›¸å¯¹è·¯å¾„)
from ..diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan
# å¯¼å…¥è¾…åŠ©å‡½æ•°
from ..diffusers_helper.utils import repeat_to_batch_size, crop_or_pad_yield_mask
from ..diffusers_helper.memory import move_model_to_device_with_memory_preservation, get_cuda_free_memory_gb

# VAEç¼©æ”¾å› å­ (Hunyuan Videoä½¿ç”¨0.476986)
vae_scaling_factor = 0.476986

class HyVideoModel(comfy.model_base.BaseModel):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.pipeline = {}
        self.load_device = model_management.get_torch_device()

    def __getitem__(self, k):
        return self.pipeline[k]

    def __setitem__(self, k, v):
        self.pipeline[k] = v


class HyVideoModelConfig:
    def __init__(self, dtype):
        self.unet_config = {}
        self.unet_extra_config = {}
        self.latent_format = comfy.latent_formats.HunyuanVideo
        self.latent_format.latent_channels = 16
        self.manual_cast_dtype = dtype
        self.sampling_settings = {"multiplier": 1.0}
        self.memory_usage_factor = 2.0
        self.unet_config["disable_unet_model_creation"] = True

class FramePackDiffusersSampler:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "fp_pipeline": ("FP_DIFFUSERS_PIPELINE",), # æ¥æ”¶æ¥è‡ªåŠ è½½èŠ‚ç‚¹çš„ Pipeline
                "positive": ("CONDITIONING",),
                "negative": ("CONDITIONING",),
                "steps": ("INT", {"default": 30, "min": 1, "max": 10000}),
                "cfg": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step": 0.1}), # real guidance scale
                "guidance_scale": ("FLOAT", {"default": 6.0, "min": 0.0, "max": 100.0, "step": 0.1}), # distilled guidance scale
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}),
                "latent_window_size": ("INT", {"default": 9, "min": 4, "max": 33, "step": 1, 
                                              "tooltip": "çª—å£å¤§å°å‚æ•°ï¼Œæ§åˆ¶æ¯ä¸ªåˆ†æ®µå¤„ç†çš„å¸§æ•°ã€‚è¾ƒå¤§çš„å€¼å¯èƒ½ç”Ÿæˆæ›´è¿è´¯çš„è§†é¢‘ï¼Œä½†éœ€è¦æ›´å¤šGPUå†…å­˜"}),
                "width": ("INT", {"default": 512, "min": 256, "max": 2048, "step": 8}),
                "height": ("INT", {"default": 512, "min": 256, "max": 2048, "step": 8}),
                "total_second_length": ("FLOAT", {"default": 5, "min": 1, "max": 60, "step": 0.1, 
                                                 "tooltip": "è§†é¢‘æ€»æ—¶é•¿(ç§’)"}),
                "gpu_memory_preservation": ("FLOAT", {"default": 6.0, "min": 0.0, "max": 100.0, "step": 0.1, 
                                                     "tooltip": "GPUå†…å­˜ä¿ç•™é‡(GB)ï¼Œè¶Šå¤§è¶Šç¨³å®šä½†é€Ÿåº¦è¶Šæ…¢"}),
                "sampler": (["unipc"],
                            {"default": "unipc", 
                             "tooltip": "é‡‡æ ·å™¨ç±»å‹ï¼Œç›®å‰ä»…æ”¯æŒunipc"}),
            },
            "optional": {
                "start_latent": ("LATENT", {"tooltip": "I2Væ¨¡å¼çš„è¾“å…¥æ½œå˜é‡ï¼Œå¯ä»VAE Encodeè·å–"}),
                "clip_vision": ("CLIP_VISION_OUTPUT", {"tooltip": "CLIP Visionçš„è¾“å‡ºï¼Œç”¨äºå›¾åƒå¼•å¯¼"}),
                "shift": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 10.0, "step": 0.1, 
                                   "tooltip": "å½±å“è¿åŠ¨å¹…åº¦ï¼Œæ•°å€¼è¶Šé«˜è¿åŠ¨è¶Šå¼º"}),
                "fps": ("INT", {"default": 30, "min": 1, "max": 60, 
                               "tooltip": "è§†é¢‘å¸§ç‡(æ¯ç§’å¸§æ•°)"}),
                "use_teacache": ("BOOLEAN", {"default": True, "tooltip": "ä½¿ç”¨teacacheåŠ é€Ÿé‡‡æ ·"}),
                "teacache_thresh": ("FLOAT", {"default": 0.15, "min": 0.0, "max": 1.0, "step": 0.01, 
                                             "tooltip": "teacacheç›¸å¯¹L1æŸå¤±é˜ˆå€¼"}),
                "denoise_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01, 
                                               "tooltip": "I2Væ¨¡å¼çš„å»å™ªå¼ºåº¦ï¼Œè¶Šä½ä¿ç•™è¶Šå¤šåŸå§‹å›¾åƒç‰¹å¾"}),
            }
        }

    RETURN_TYPES = ("LATENT",)
    FUNCTION = "sample"
    CATEGORY = "FramePack"

    def sample(self, fp_pipeline, positive, negative, steps, cfg, guidance_scale, seed,
               latent_window_size, width, height, total_second_length, gpu_memory_preservation,
               sampler, start_latent=None, clip_vision=None, shift=0.0, fps=8, use_teacache=True, 
               teacache_thresh=0.15, denoise_strength=1.0):

        # ç¡®ä¿å°ºå¯¸è¶³å¤Ÿå¤§
        if height < 256 or width < 256:
            raise ValueError(f"è¾“å…¥å°ºå¯¸å¤ªå°: {width}x{height}ï¼Œè¯·ç¡®ä¿å®½åº¦å’Œé«˜åº¦è‡³å°‘ä¸º256åƒç´ ")
        
        # ç¡®ä¿æˆ‘ä»¬æœ‰ä¸€ä¸ªåŠ è½½å¥½çš„transformer
        if "transformer" not in fp_pipeline or "dtype" not in fp_pipeline:
            raise ValueError("æ— æ•ˆçš„Pipelineå¯¹è±¡ã€‚è¯·ä½¿ç”¨Load FramePack PipelineèŠ‚ç‚¹åŠ è½½æœ‰æ•ˆçš„æ¨¡å‹ã€‚")
        
        transformer = fp_pipeline["transformer"]
        dtype = fp_pipeline["dtype"]
        
        # è®¾å¤‡å’Œæ•°æ®ç±»å‹å‡†å¤‡
        device = model_management.get_torch_device()
        offload_device = model_management.unet_offload_device()
        print(f"[FramePack Sampler] ä½¿ç”¨è®¾å¤‡: {device}, ç²¾åº¦: {dtype}")
        
        # è®¡ç®—æ½œå˜é‡å°ºå¯¸
        latent_height = height // 8
        latent_width = width // 8
        
        # è®¡ç®—è§†é¢‘å¸§æ•°å’Œåˆ†æ®µ
        num_frames_per_window = latent_window_size * 4 - 3  # æ¯ä¸ªçª—å£å¯ç”Ÿæˆçš„æœ‰æ•ˆå¸§æ•°
        total_latent_sections = (total_second_length * fps) / num_frames_per_window
        total_latent_sections = int(max(round(total_latent_sections), 1))
        print(f"[FramePack Sampler] æ€»åˆ†æ®µæ•°: {total_latent_sections}, æ¯æ®µå¸§æ•°: {num_frames_per_window}")
        
        # å†…å­˜ç®¡ç†
        model_management.unload_all_models()
        model_management.cleanup_models()
        model_management.soft_empty_cache()
        
        # å¤„ç†æ¡ä»¶è¾“å…¥
        print("[FramePack Sampler] å¤„ç†æ¡ä»¶è¾“å…¥...")
        
        # å¤„ç†æ­£å‘æ¡ä»¶
        llama_vec = positive[0][0].to(dtype=dtype, device=device)
        clip_l_pooler = positive[0][1]["pooled_output"].to(dtype=dtype, device=device)
        
        # å¤„ç†è´Ÿå‘æ¡ä»¶
        if not math.isclose(cfg, 1.0):  # å¦‚æœéœ€è¦çœŸæ­£çš„CFG
            llama_vec_n = negative[0][0].to(dtype=dtype, device=device)
            clip_l_pooler_n = negative[0][1]["pooled_output"].to(dtype=dtype, device=device)
        else:
            # å¦‚æœCFGä¸º1.0ï¼Œåˆ›å»ºå…¨é›¶æ¡ä»¶
            llama_vec_n = torch.zeros_like(llama_vec, device=device)
            clip_l_pooler_n = torch.zeros_like(clip_l_pooler, device=device)
        
        # è£å‰ªæˆ–å¡«å……LLAMAåµŒå…¥å’Œåˆ›å»ºæ³¨æ„åŠ›æ©ç 
        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)
        llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)
        
        # å‡†å¤‡CLIPè§†è§‰ç‰¹å¾
        image_embeddings = None
        if clip_vision is not None:
            image_embeddings = clip_vision["last_hidden_state"].to(dtype=dtype, device=device)
            print(f"[FramePack Sampler] CLIPè§†è§‰ç‰¹å¾å½¢çŠ¶: {image_embeddings.shape}")
        
        # å‡†å¤‡åˆå§‹æ½œå˜é‡
        batch_size = 1
        initial_latent = None
        
        # å¦‚æœæä¾›äº†èµ·å§‹æ½œå˜é‡(I2Væ¨¡å¼)
        if start_latent is not None:
            print("[FramePack Sampler] å‡†å¤‡I2Væ¨¡å¼çš„èµ·å§‹æ½œå˜é‡...")
            # è·å–æ½œå˜é‡å¹¶åº”ç”¨VAEç¼©æ”¾å› å­
            initial_latent = start_latent["samples"] * vae_scaling_factor
            
            # è¯¦ç»†æ‰“å°æ½œå˜é‡çš„å½¢çŠ¶ä¿¡æ¯ç”¨äºè°ƒè¯•
            print(f"[FramePack Sampler] åŸå§‹æ½œå˜é‡å½¢çŠ¶: {initial_latent.shape}, ç±»å‹: {initial_latent.dtype}")
            
            # æ£€æŸ¥æ½œå˜é‡ç»´åº¦æ˜¯å¦æ­£ç¡®
            if initial_latent.ndim == 4:  # å•å¸§æ½œå˜é‡ [B, C, H, W]
                # æ·»åŠ æ—¶é—´ç»´åº¦ [B, C, 1, H, W]
                initial_latent = initial_latent.unsqueeze(2)
                print(f"[FramePack Sampler] æ·»åŠ æ—¶é—´ç»´åº¦åå½¢çŠ¶: {initial_latent.shape}")
            
            # ç¡®è®¤å½¢çŠ¶æ˜¯5ç»´ [B, C, T, H, W]
            if initial_latent.ndim != 5:
                raise ValueError(f"è¾“å…¥æ½œå˜é‡å½¢çŠ¶é”™è¯¯: {initial_latent.shape}ï¼Œåº”ä¸º [B, C, T, H, W] æˆ– [B, C, H, W]")
            
            # å®‰å…¨åœ°è·å–è°ƒæ•´ç›®æ ‡å°ºå¯¸
            current_height = initial_latent.shape[3]
            current_width = initial_latent.shape[4]
            
            # è°ƒæ•´æ½œå˜é‡å°ºå¯¸
            if current_height != latent_height or current_width != latent_width:
                print(f"[FramePack Sampler] è°ƒæ•´æ½œå˜é‡å°ºå¯¸ä» {current_height}x{current_width} åˆ° {latent_height}x{latent_width}")
                
                # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•é‡å¡‘å’Œè°ƒæ•´å°ºå¯¸
                batch, channels, frames = initial_latent.shape[0], initial_latent.shape[1], initial_latent.shape[2]
                
                # å…ˆå±•å¹³æ‰€æœ‰å¸§è¿›è¡Œå¤„ç†
                flattened = initial_latent.reshape(batch * channels * frames, 1, current_height, current_width)
                print(f"[FramePack Sampler] å±•å¹³åå½¢çŠ¶: {flattened.shape}")
                
                # åº”ç”¨æ’å€¼
                resized = torch.nn.functional.interpolate(
                    flattened, 
                    size=(latent_height, latent_width),
                    mode='bilinear',
                    align_corners=False
                )
                print(f"[FramePack Sampler] è°ƒæ•´å°ºå¯¸åå½¢çŠ¶: {resized.shape}")
                
                # é‡å¡‘å›åŸå§‹ç»´åº¦ç»“æ„
                initial_latent = resized.reshape(batch, channels, frames, latent_height, latent_width)
                print(f"[FramePack Sampler] é‡å¡‘åæœ€ç»ˆå½¢çŠ¶: {initial_latent.shape}")
            
            # å°†æ½œå˜é‡ç§»åŠ¨åˆ°è®¡ç®—è®¾å¤‡
            initial_latent = initial_latent.to(device=device, dtype=dtype)
            print(f"[FramePack Sampler] æœ€ç»ˆèµ·å§‹æ½œå˜é‡å½¢çŠ¶: {initial_latent.shape}")
        
        # åˆå§‹åŒ–èµ·å§‹æ½œå˜é‡(å¦‚æœæ˜¯T2Væ¨¡å¼æˆ–æ²¡æœ‰æä¾›èµ·å§‹æ½œå˜é‡)
        if initial_latent is None:
            start_latent = torch.zeros(
                (batch_size, 16, 1, latent_height, latent_width),
                dtype=torch.float32,
                device="cpu"
            )
        else:
            # å¦‚æœæœ‰èµ·å§‹æ½œå˜é‡ï¼Œç”¨å®ƒåˆå§‹åŒ–start_latent
            start_latent = initial_latent.detach().cpu().to(torch.float32)
            if start_latent.shape[2] > 1:
                # åªå–ç¬¬ä¸€å¸§
                start_latent = start_latent[:, :, :1, :, :]
        
        # åˆå§‹åŒ–å†å²æ½œå˜é‡
        history_latents = torch.zeros(
            (batch_size, 16, 1 + 2 + 16, latent_height, latent_width),
            dtype=torch.float32, 
            device="cpu"
        )
        
        # å‡†å¤‡éšæœºç”Ÿæˆå™¨
        generator = torch.Generator("cpu").manual_seed(seed)
        
        # åˆ›å»ºComfyUIæ¨¡å‹å°è£…
        comfy_model = HyVideoModel(
            HyVideoModelConfig(dtype),
            model_type=comfy.model_base.ModelType.FLOW,
            device=device,
        )
        
        # åˆ›å»ºæ¨¡å‹patcher
        patcher = comfy.model_patcher.ModelPatcher(comfy_model, device, torch.device("cpu"))
        
        # åˆ›å»ºè¿›åº¦æ¡å›è°ƒå‡½æ•° - ä¿®æ”¹ä¸ºæ›´å‡†ç¡®åœ°åæ˜ å¤šåˆ†æ®µè¿›åº¦
        # è®¡ç®—æ€»æ­¥æ•°ä¸º steps * total_latent_sections
        total_steps = steps * total_latent_sections
        progress_bar = comfy.utils.ProgressBar(total_steps)
        
        # è®°å½•å½“å‰åˆ†æ®µç´¢å¼•å’Œå·²å®Œæˆçš„åˆ†æ®µæ•°
        current_section_index = 0
        
        # å®šä¹‰ä¸€ä¸ªé€‚é…k_diffusionåº“è°ƒç”¨æ ¼å¼çš„å›è°ƒå‡½æ•°
        def callback_adapter(d):
            # k_diffusionçš„callbackä¼ å…¥å‚æ•°æ˜¯ä¸€ä¸ªå­—å…¸: {'x': x, 'i': i, 'denoised': model_prev_list[-1]}
            if 'i' in d:
                step = d['i']
                # åªæ›´æ–°ä¸€æ­¥ï¼Œå› ä¸ºæ€»æ­¥æ•°å·²ç»æ˜¯è€ƒè™‘äº†æ‰€æœ‰åˆ†æ®µçš„
                progress_bar.update(1)
                
                # æ‰“å°æ›´è¯¦ç»†çš„è¿›åº¦ä¿¡æ¯
                if step % 5 == 0 or step == steps - 1:  # æ¯5æ­¥æˆ–æœ€åä¸€æ­¥æ‰“å°ä¸€æ¬¡
                    section_progress = f"{current_section_index + 1}/{total_latent_sections}"
                    overall_progress = f"{(current_section_index * steps + step + 1)}/{total_steps}"
                    print(f"[FramePack Sampler] è¿›åº¦: åˆ†æ®µ {section_progress}, æ­¥éª¤ {step + 1}/{steps}, æ€»è¿›åº¦ {overall_progress}")
            return None
            
        # ---------- æ”¹è¿›çš„æ¨¡å‹åŠ è½½ä¸å†…å­˜ç®¡ç† ----------
        print(f"[FramePack Sampler] å¼€å§‹åŠ è½½æ¨¡å‹åˆ°GPUè®¾å¤‡ï¼Œå†…å­˜ä¿ç•™é‡è®¾ç½®ä¸º {gpu_memory_preservation} GB")
        
        # æ£€æŸ¥å¯ç”¨å†…å­˜å¹¶ä¼°ç®—æ¨¡å‹å¤§å°
        try:
            current_free_memory = get_cuda_free_memory_gb(device)
            print(f"[FramePack Sampler] å½“å‰GPUå¯ç”¨å†…å­˜: {current_free_memory:.2f} GB")
            
            # å¦‚æœå†…å­˜ä¿ç•™å€¼å¤§äºå½“å‰å¯ç”¨å†…å­˜çš„80%ï¼Œå‘å‡ºè­¦å‘Šå¹¶è°ƒæ•´
            if gpu_memory_preservation > current_free_memory * 0.8:
                adjusted_preservation = current_free_memory * 0.5  # è°ƒæ•´ä¸ºå¯ç”¨å†…å­˜çš„50%
                print(f"[FramePack Sampler] è­¦å‘Š: å†…å­˜ä¿ç•™å€¼({gpu_memory_preservation}GB)è¿‡å¤§, è‡ªåŠ¨è°ƒæ•´ä¸º {adjusted_preservation:.2f}GB")
                gpu_memory_preservation = adjusted_preservation
            
            # åœ¨åŠ è½½å‰å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜
            if current_free_memory <= gpu_memory_preservation + 1.0:  # éœ€è¦è‡³å°‘ä¿ç•™å€¼+1GB
                print(f"[FramePack Sampler] è­¦å‘Š: GPUå†…å­˜ä¸è¶³! å¯ç”¨: {current_free_memory:.2f}GB, éœ€è¦: >{gpu_memory_preservation+1.0}GB")
                print(f"[FramePack Sampler] å°è¯•é™ä½é‡‡æ ·åˆ†è¾¨ç‡æˆ–é™ä½ä¿ç•™å†…å­˜å€¼")
        except Exception as e:
            print(f"[FramePack Sampler] å†…å­˜æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}")
            print("[FramePack Sampler] ç»§ç»­æ‰§è¡Œï¼Œä½†å¯èƒ½ä¸ç¨³å®š")
        
        # æ”¹è¿›çš„æ¨¡å‹åŠ è½½æ–¹æ³•
        try:
            # åˆ†é˜¶æ®µåŠ è½½æ¨¡å‹ï¼Œæ¯é˜¶æ®µæ£€æŸ¥å†…å­˜
            print(f"[FramePack Sampler] é˜¶æ®µ1: å°†æ¨¡å‹ç§»åŠ¨è‡³ {device}...")
            move_model_to_device_with_memory_preservation(
                transformer, 
                target_device=device, 
                preserved_memory_gb=gpu_memory_preservation
            )
            
            # æ£€æŸ¥åŠ è½½åçš„å†…å­˜çŠ¶æ€
            try:
                post_load_memory = get_cuda_free_memory_gb(device)
                print(f"[FramePack Sampler] æ¨¡å‹åŠ è½½åGPUå¯ç”¨å†…å­˜: {post_load_memory:.2f} GB")
                
                if post_load_memory < gpu_memory_preservation:
                    print(f"[FramePack Sampler] æ³¨æ„: åŠ è½½åå¯ç”¨å†…å­˜({post_load_memory:.2f}GB)ä½äºä¿ç•™ç›®æ ‡({gpu_memory_preservation}GB)")
                    print(f"[FramePack Sampler] å°†å°è¯•ç»§ç»­è¿è¡Œï¼Œä½†å¯èƒ½ä¼šå‡ºç°å†…å­˜ä¸è¶³é”™è¯¯")
            except Exception as e:
                print(f"[FramePack Sampler] åŠ è½½åå†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
        
        except Exception as e:
            print(f"[FramePack Sampler] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(f"[FramePack Sampler] å°è¯•ä½¿ç”¨å¤‡ç”¨åŠ è½½æ–¹æ³•...")
            
            # å¤‡ç”¨åŠ è½½æ–¹æ³• - ç›´æ¥åŠ è½½ä½†ä¸ç®¡ç†å†…å­˜
            transformer.to(device)
            print("[FramePack Sampler] ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½æ¨¡å‹å®Œæˆ")
        
        # è¿è¡Œé‡‡æ ·
        print("[FramePack Sampler] å¼€å§‹é‡‡æ ·...")
        print(f"  - å°ºå¯¸: {width}x{height}, æ€»å¸§æ•°: {total_second_length * fps}")
        print(f"  - åˆ†æ®µæ•°: {total_latent_sections}, æ¯æ®µçª—å£å¤§å°: {latent_window_size}")
        print(f"  - æ­¥æ•°: {steps}, CFG: {cfg}, Guidance Scale: {guidance_scale}")
        print(f"  - ç§å­: {seed}, ç§»ä½: {shift}")
        if initial_latent is not None:
            print(f"  - I2Væ¨¡å¼: å»å™ªå¼ºåº¦ = {denoise_strength}")
        
        try:
            # å¤„ç†åˆ†æ®µç”Ÿæˆ
            total_generated_latent_frames = 0
            latent_paddings_list = list(reversed(range(total_latent_sections)))
            latent_paddings = latent_paddings_list.copy()
            
            # å¯¹äºé•¿è§†é¢‘ï¼Œä¼˜åŒ–åˆ†æ®µç­–ç•¥
            if total_latent_sections > 4:
                latent_paddings = [3] + [2] * (total_latent_sections - 3) + [1, 0]
                latent_paddings_list = latent_paddings.copy()
            
            # é€æ®µç”Ÿæˆ
            for latent_padding in latent_paddings:
                print(f"[FramePack Sampler] ç”Ÿæˆåˆ†æ®µ {latent_padding + 1}/{total_latent_sections}")
                is_last_section = latent_padding == 0
                latent_padding_size = latent_padding * latent_window_size
                
                print(f'  - å¡«å……å¤§å° = {latent_padding_size}, æ˜¯å¦æœ€ååˆ†æ®µ = {is_last_section}')
                
                # åˆ›å»ºå’Œåˆ†å‰²ç´¢å¼•
                indices = torch.arange(0, sum([1, latent_padding_size, latent_window_size, 1, 2, 16])).unsqueeze(0)
                clean_latent_indices_pre, blank_indices, latent_indices, clean_latent_indices_post, clean_latent_2x_indices, clean_latent_4x_indices = indices.split(
                    [1, latent_padding_size, latent_window_size, 1, 2, 16], dim=1
                )
                clean_latent_indices = torch.cat([clean_latent_indices_pre, clean_latent_indices_post], dim=1)
                
                # å‡†å¤‡æ¸…ç†æ½œå˜é‡
                clean_latents_pre = start_latent.to(history_latents)
                clean_latents_post, clean_latents_2x, clean_latents_4x = history_latents[:, :, :1 + 2 + 16, :, :].split([1, 2, 16], dim=2)
                clean_latents = torch.cat([clean_latents_pre, clean_latents_post], dim=2)
                
                # è®¾ç½®teacache
                if hasattr(transformer, 'initialize_teacache'):
                    if use_teacache:
                        transformer.initialize_teacache(enable_teacache=True, num_steps=steps, rel_l1_thresh=teacache_thresh)
                    else:
                        transformer.initialize_teacache(enable_teacache=False)
                
                # æ£€æŸ¥å½“å‰å†…å­˜çŠ¶æ€
                try:
                    current_mem = get_cuda_free_memory_gb(device)
                    print(f"[FramePack Sampler] é‡‡æ ·å‰GPUå¯ç”¨å†…å­˜: {current_mem:.2f} GB")
                    if current_mem < 1.0:  # å¦‚æœå†…å­˜ä¸¥é‡ä¸è¶³
                        print("[FramePack Sampler] è­¦å‘Š: GPUå†…å­˜ä¸¥é‡ä¸è¶³ï¼Œå°è¯•é‡Šæ”¾éƒ¨åˆ†å†…å­˜...")
                        torch.cuda.empty_cache()
                except Exception as e:
                    print(f"[FramePack Sampler] å†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
                
                # æ‰§è¡Œé‡‡æ ·
                with torch.autocast(device_type=model_management.get_autocast_device(device), dtype=dtype, enabled=True):
                    # æ›´æ–°å½“å‰åˆ†æ®µç´¢å¼•
                    current_section_index = total_latent_sections - latent_padding - 1
                    print(f"[FramePack Sampler] å¼€å§‹å¤„ç†åˆ†æ®µ {current_section_index + 1}/{total_latent_sections}")
                    
                    generated_latents = sample_hunyuan(
                        transformer=transformer,
                        sampler=sampler,  # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„é‡‡æ ·å™¨
                        initial_latent=initial_latent,  # ä½¿ç”¨åˆå§‹æ½œå˜é‡(I2Væ¨¡å¼)
                        concat_latent=None,
                        strength=denoise_strength,  # åº”ç”¨å»å™ªå¼ºåº¦
                        width=width,
                        height=height,
                        frames=num_frames_per_window,
                        real_guidance_scale=cfg,
                        distilled_guidance_scale=guidance_scale,
                        guidance_rescale=0.0,
                        shift=shift if shift > 0 else None,
                        num_inference_steps=steps,
                        batch_size=batch_size,
                        generator=generator,
                        prompt_embeds=llama_vec,
                        prompt_embeds_mask=llama_attention_mask,
                        prompt_poolers=clip_l_pooler,
                        negative_prompt_embeds=llama_vec_n,
                        negative_prompt_embeds_mask=llama_attention_mask_n,
                        negative_prompt_poolers=clip_l_pooler_n,
                        dtype=dtype,
                        device=device,
                        negative_kwargs=None,
                        callback=callback_adapter,  # ä½¿ç”¨æˆ‘ä»¬çš„é€‚é…å™¨å›è°ƒå‡½æ•°
                        # æ·»åŠ é¢å¤–å‚æ•°
                        image_embeddings=image_embeddings,
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                    )
                
                # æ£€æŸ¥é‡‡æ ·åçš„å†…å­˜çŠ¶æ€
                try:
                    post_sample_mem = get_cuda_free_memory_gb(device)
                    print(f"[FramePack Sampler] åˆ†æ®µé‡‡æ ·åGPUå¯ç”¨å†…å­˜: {post_sample_mem:.2f} GB")
                    
                    # å¦‚æœå†…å­˜ä½äºä¿ç•™å€¼çš„50%ï¼Œæ¸…ç†ç¼“å­˜
                    if post_sample_mem < gpu_memory_preservation * 0.5:
                        print("[FramePack Sampler] å†…å­˜ä¸è¶³ï¼Œæ¸…ç†ç¼“å­˜...")
                        torch.cuda.empty_cache()
                except Exception as e:
                    print(f"[FramePack Sampler] é‡‡æ ·åå†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
                
                # å¦‚æœæ˜¯æœ€åä¸€æ®µï¼Œè¿æ¥èµ·å§‹æ½œå˜é‡
                if is_last_section:
                    generated_latents = torch.cat([start_latent.to(generated_latents), generated_latents], dim=2)
                
                # æ›´æ–°æ€»å¸§æ•°å’Œå†å²æ½œå˜é‡
                total_generated_latent_frames += int(generated_latents.shape[2])
                history_latents = torch.cat([generated_latents.to(history_latents), history_latents], dim=2)
                
                # è·å–å®é™…ç”Ÿæˆçš„æ½œå˜é‡
                real_history_latents = history_latents[:, :, :total_generated_latent_frames, :, :]
                
                # å¦‚æœæ˜¯æœ€åä¸€æ®µï¼Œåœæ­¢ç”Ÿæˆ
                if is_last_section:
                    break
            
            print("[FramePack Sampler] é‡‡æ ·å®Œæˆ.")
            print(f"[FramePack Sampler] è¾“å‡ºå½¢çŠ¶: {real_history_latents.shape}")
            
            # è®°å½•å®é™…è°ƒç”¨çš„é‡‡æ ·ä¿¡æ¯
            print(f"[FramePack Sampler] ğŸ“Š é‡‡æ ·ä¿¡æ¯: çª—å£å¤§å°={latent_window_size}, å¸§æ•°/çª—å£={num_frames_per_window}")
            
            # ç¡®ä¿è¿›åº¦æ¡æ˜¾ç¤ºä¸ºå®ŒæˆçŠ¶æ€
            progress_remaining = total_steps - (current_section_index + 1) * steps
            if progress_remaining > 0:
                print(f"[FramePack Sampler] æ›´æ–°è¿›åº¦æ¡åˆ°å®ŒæˆçŠ¶æ€ (å‰©ä½™ {progress_remaining} æ­¥)")
                progress_bar.update(progress_remaining)
            print("[FramePack Sampler] âœ… è¿›åº¦: 100% å®Œæˆ!")
            
            # è¿”å›ç»“æœï¼Œåº”ç”¨VAEç¼©æ”¾å› å­
            return ({"samples": real_history_latents.to(model_management.intermediate_device()) / vae_scaling_factor},)
            
        except Exception as e:
            # è¯¦ç»†è®°å½•é”™è¯¯ä¿¡æ¯
            error_message = f"[FramePack Sampler] âŒ é”™è¯¯: {str(e)}"
            print(error_message)
            print("[FramePack Sampler] ğŸ“‹ é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            
            # æ›´æ–°è¿›åº¦æ¡åˆ°é”™è¯¯çŠ¶æ€
            try:
                # è®¡ç®—å‰©ä½™æ­¥æ•°å¹¶æ›´æ–°è¿›åº¦æ¡
                if 'progress_bar' in locals() and 'current_section_index' in locals() and 'total_steps' in locals():
                    progress_so_far = min(current_section_index * steps, total_steps)
                    progress_remaining = total_steps - progress_so_far
                    if progress_remaining > 0:
                        print(f"[FramePack Sampler] ç”±äºé”™è¯¯æ›´æ–°è¿›åº¦æ¡ (å‰©ä½™ {progress_remaining} æ­¥)")
                        progress_bar.update(progress_remaining)
                print("[FramePack Sampler] âš ï¸ è¿›åº¦: ç”±äºé”™è¯¯è€Œä¸­æ–­!")
            except Exception as progress_error:
                print(f"[FramePack Sampler] æ›´æ–°è¿›åº¦æ¡å¤±è´¥: {progress_error}")
            
            # æä¾›é€šç”¨çš„å»ºè®®è§£å†³æ–¹æ¡ˆ
            print("[FramePack Sampler] ğŸ”§ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("1. æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿï¼Œå¯èƒ½éœ€è¦é™ä½åˆ†è¾¨ç‡æˆ–å‡å°‘ç”Ÿæˆçš„å¸§æ•°")
            print("2. ç¡®ä¿æ¨¡å‹æ­£ç¡®åŠ è½½")
            print("3. æ£€æŸ¥æ¡ä»¶è¾“å…¥æ˜¯å¦æœ‰æ•ˆ")
            print("4. å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯ä»¥å°è¯•é‡å¯ComfyUIæˆ–æ¸…ç†ç¼“å­˜")
            
            # åˆ›å»ºä¸€ä¸ªç©ºçš„æœ‰æ•ˆæ½œå˜é‡ä½œä¸ºè¿”å›å€¼
            try:
                print("[FramePack Sampler] åˆ›å»ºç©ºæ½œå˜é‡ä½œä¸ºé”™è¯¯æ¢å¤...")
                # åˆ›å»ºä¸€ä¸ªå°çš„ç©ºæ½œå˜é‡
                empty_latent = torch.zeros(
                    (1, 16, 1, height // 8, width // 8), 
                    dtype=torch.float32,
                    device=model_management.intermediate_device()
                )
                return ({"samples": empty_latent},)
            except Exception as fallback_error:
                print(f"[FramePack Sampler] åˆ›å»ºç©ºæ½œå˜é‡å¤±è´¥: {fallback_error}")
                # æœ€å°çš„å¯èƒ½æ½œå˜é‡
                minimal_latent = torch.zeros(
                    (1, 4, 1, 8, 8), 
                    dtype=torch.float32,
                    device=model_management.intermediate_device()
                )
                return ({"samples": minimal_latent},)
        finally:
            # ä¸»åŠ¨é‡Šæ”¾å†…å­˜
            print("[FramePack Sampler] ä¸»åŠ¨æ¸…ç†GPUå†…å­˜...")
            torch.cuda.empty_cache()
            
            # é‡Šæ”¾transformer
            try:
                print(f"[FramePack Sampler] å°†transformerå¸è½½åˆ° {offload_device}")
                transformer.to(offload_device)
            except Exception as e:
                print(f"[FramePack Sampler] å¸è½½transformeræ—¶å‡ºé”™: {e}")
            
            model_management.soft_empty_cache()


NODE_CLASS_MAPPINGS = {
    "FramePackDiffusersSampler_HY": FramePackDiffusersSampler
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "FramePackDiffusersSampler_HY": "FramePack Sampler (HY)"
} 