# ComfyUI-FramePack-HY/nodes/sampler.py

import torch
import math
import numpy as np
from tqdm import tqdm
import random
import traceback # Import traceback for better error logging

# å¯¼å…¥ ComfyUI ç›¸å…³
import comfy.model_management as model_management
import comfy.utils
import comfy.model_base
import comfy.latent_formats
import comfy.model_patcher

# å¯¼å…¥è‡ªå®šä¹‰é‡‡æ ·å‡½æ•° (ä½¿ç”¨ç›¸å¯¹è·¯å¾„)
from ..diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan
# å¯¼å…¥è¾…åŠ©å‡½æ•°
from ..diffusers_helper.utils import repeat_to_batch_size, crop_or_pad_yield_mask
from ..diffusers_helper.memory import move_model_to_device_with_memory_preservation, get_cuda_free_memory_gb

# VAEç¼©æ”¾å› å­ (Hunyuan Videoä½¿ç”¨0.476986)
vae_scaling_factor = 0.476986

class HyVideoModel(comfy.model_base.BaseModel):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.pipeline = {}
        self.load_device = model_management.get_torch_device()

    def __getitem__(self, k):
        return self.pipeline[k]

    def __setitem__(self, k, v):
        self.pipeline[k] = v


class HyVideoModelConfig:
    def __init__(self, dtype):
        self.unet_config = {}
        self.unet_extra_config = {}
        self.latent_format = comfy.latent_formats.HunyuanVideo
        self.latent_format.latent_channels = 16
        self.manual_cast_dtype = dtype
        self.sampling_settings = {"multiplier": 1.0}
        self.memory_usage_factor = 2.0
        self.unet_config["disable_unet_model_creation"] = True

class FramePackDiffusersSampler:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "fp_pipeline": ("FP_DIFFUSERS_PIPELINE",), # æ¥æ”¶æ¥è‡ªåŠ è½½èŠ‚ç‚¹çš„ Pipeline
                "positive": ("CONDITIONING",),
                "negative": ("CONDITIONING",),
                "steps": ("INT", {"default": 30, "min": 1, "max": 10000}),
                "cfg": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 100.0, "step": 0.1}), # real guidance scale
                "guidance_scale": ("FLOAT", {"default": 6.0, "min": 0.0, "max": 100.0, "step": 0.1}), # distilled guidance scale
                "seed": ("INT", {"default": 66666666, "min": 0, "max": 0xffffffffffffffff}),
                "width": ("INT", {"default": 512, "min": 256, "max": 2048, "step": 8}),
                "height": ("INT", {"default": 512, "min": 256, "max": 2048, "step": 8}),
                "gpu_memory_preservation": ("FLOAT", {"default": 6.0, "min": 0.0, "max": 100.0, "step": 0.1, 
                                                     "tooltip": "GPUå†…å­˜ä¿ç•™é‡(GB)ï¼Œè¶Šå¤§è¶Šç¨³å®šä½†é€Ÿåº¦è¶Šæ…¢"}),
                "sampler": (["unipc"],
                            {"default": "unipc", 
                             "tooltip": "é‡‡æ ·å™¨ç±»å‹ï¼Œç›®å‰ä»…æ”¯æŒunipc"}),
            },
            "optional": {
                # æ¥æ”¶æ¥è‡ªCreateKeyframesçš„å‚æ•°
                # "total_second_length": ("FLOAT", {"default": 5, "min": 1, "max": 60, "step": 0.1, 
                #                                  "tooltip": "è§†é¢‘æ€»æ—¶é•¿(ç§’)ï¼Œä¼˜å…ˆä½¿ç”¨keyframesèŠ‚ç‚¹è¿æ¥çš„å€¼"}),
                # "fps": ("INT", {"default": 24, "min": 1, "max": 60, 
                #                "tooltip": "è§†é¢‘å¸§ç‡(æ¯ç§’å¸§æ•°)ï¼Œä¼˜å…ˆä½¿ç”¨keyframesèŠ‚ç‚¹è¿æ¥çš„å€¼"}),
                # "latent_window_size": ("INT", {"default": 9, "min": 4, "max": 33, "step": 1, 
                #                               "tooltip": "çª—å£å¤§å°å‚æ•°ï¼Œæ§åˆ¶æ¯ä¸ªåˆ†æ®µå¤„ç†çš„å¸§æ•°ï¼Œä¼˜å…ˆä½¿ç”¨keyframesèŠ‚ç‚¹è¿æ¥çš„å€¼"}),
                
                # ä»CreateKeyframesèŠ‚ç‚¹ç›´æ¥è¿æ¥çš„è§†é¢‘å‚æ•°
                "video_length_seconds": ("video_length_seconds", {"default": None, 
                                                  "tooltip": "ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„è§†é¢‘æ—¶é•¿(ç§’)ï¼Œä¼˜å…ˆçº§æœ€é«˜"}),
                "video_fps": ("video_fps", {"default": None,  
                                     "tooltip": "ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„å¸§ç‡(fps)ï¼Œä¼˜å…ˆçº§æœ€é«˜"}),
                "window_size": ("window_size", {"default": None, 
                                       "tooltip": "ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„çª—å£å¤§å°ï¼Œä¼˜å…ˆçº§æœ€é«˜"}),
                
                "start_latent": ("LATENT", {"tooltip": "I2Væ¨¡å¼çš„è¾“å…¥æ½œå˜é‡ï¼Œå¯ä»VAE Encodeè·å–"}),
                "clip_vision": ("CLIP_VISION_OUTPUT", {"tooltip": "CLIP Visionçš„è¾“å‡ºï¼Œç”¨äºå›¾åƒå¼•å¯¼"}),
                "shift": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 10.0, "step": 0.1, 
                                   "tooltip": "å½±å“è¿åŠ¨å¹…åº¦ï¼Œæ•°å€¼è¶Šé«˜è¿åŠ¨è¶Šå¼º"}),
                "use_teacache": ("BOOLEAN", {"default": True, "tooltip": "ä½¿ç”¨teacacheåŠ é€Ÿé‡‡æ ·"}),
                "teacache_thresh": ("FLOAT", {"default": 0.15, "min": 0.0, "max": 1.0, "step": 0.01, 
                                             "tooltip": "teacacheç›¸å¯¹L1æŸå¤±é˜ˆå€¼"}),
                "denoise_strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01, 
                                               "tooltip": "I2Væ¨¡å¼çš„å»å™ªå¼ºåº¦ï¼Œè¶Šä½ä¿ç•™è¶Šå¤šåŸå§‹å›¾åƒç‰¹å¾"}),
                # å…³é”®å¸§ç›¸å…³å‚æ•°
                "keyframes": ("LATENT", {"tooltip": "ç”¨äºå¼•å¯¼è§†é¢‘å†…å®¹çš„å…³é”®å¸§æ½œå˜é‡é›†åˆ"}),
                "keyframe_indices": ("KEYFRAME_INDICES", {"tooltip": "ä¸keyframeså¯¹åº”çš„åˆ†æ®µç´¢å¼•åˆ—è¡¨ï¼ˆå¿…é¡»å‡åºï¼Œä¾‹å¦‚: '0,5,10'ï¼‰ã€‚æŒ‡å®šå…³é”®å¸§å‡ºç°åœ¨è§†é¢‘ä¸­åˆ†æ®µçš„ä½ç½®"}),
                "keyframe_guidance_strength": ("FLOAT", {"default": 1.5, "min": 0.1, "max": 10.0, "step": 0.1, 
                                                         "tooltip": "å…³é”®å¸§å¼•å¯¼å¼ºåº¦ã€‚æ§åˆ¶å…³é”®å¸§å¯¹è§†é¢‘çš„å½±å“ç¨‹åº¦ã€‚å€¼è¶Šé«˜ï¼Œè§†é¢‘åœ¨å…³é”®å¸§ä½ç½®è¶Šæ¥è¿‘ç›®æ ‡å›¾åƒï¼Œè¿‡æ¸¡æ•ˆæœè¶Šæ˜æ˜¾"})
            }
        }

    RETURN_TYPES = ("LATENT",)
    FUNCTION = "sample"
    CATEGORY = "FramePack"

    def sample(self, fp_pipeline, positive, negative, steps, cfg, guidance_scale, seed,
               width, height, gpu_memory_preservation, sampler, 
               total_second_length=5, fps=24, latent_window_size=9,
               video_length_seconds=None, video_fps=None, window_size=None,
               start_latent=None, clip_vision=None, shift=0.0, use_teacache=True, 
               teacache_thresh=0.15, denoise_strength=1.0, keyframes=None, keyframe_indices="", 
               keyframe_guidance_strength=1.5):

        # ä¼˜å…ˆä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è¿æ¥çš„è§†é¢‘å‚æ•°
        if video_length_seconds is not None:
            total_second_length = video_length_seconds
            print(f"[FramePack Sampler] ä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„è§†é¢‘æ—¶é•¿: {total_second_length}ç§’")
            
        if video_fps is not None:
            fps = video_fps
            print(f"[FramePack Sampler] ä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„å¸§ç‡: {fps}fps")
            
        if window_size is not None:
            latent_window_size = window_size
            print(f"[FramePack Sampler] ä½¿ç”¨ä»CreateKeyframesèŠ‚ç‚¹è·å–çš„çª—å£å¤§å°: {latent_window_size}")
        
        print(f"[FramePack Sampler] æœ€ç»ˆè§†é¢‘å‚æ•°: æ€»æ—¶é•¿={total_second_length}ç§’, å¸§ç‡={fps}fps, çª—å£å¤§å°={latent_window_size}")
        
        # ä¿ç•™ä½¿ç”¨keyframeä½œä¸ºstart_latentçš„åŠŸèƒ½
        # å½“åªæœ‰ä¸€ä¸ªkeyframeä¸”æ²¡æœ‰æä¾›start_latentæ—¶ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªkeyframeä½œä¸ºstart_latent
        if start_latent is None and keyframes is not None and "samples" in keyframes:
            kf_samples = keyframes["samples"]
            if kf_samples.ndim == 5 and kf_samples.shape[2] >= 1:  # è‡³å°‘æœ‰ä¸€ä¸ªå…³é”®å¸§
                print("[FramePack Sampler] æ²¡æœ‰æä¾›start_latentï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®å¸§ä½œä¸ºèµ·å§‹å¸§")
                first_frame = kf_samples[:, :, 0:1, :, :].clone()  # åªå–ç¬¬ä¸€ä¸ªå…³é”®å¸§
                start_latent = {"samples": first_frame}
                print(f"[FramePack Sampler] ä»keyframesæå–èµ·å§‹å¸§ï¼Œå½¢çŠ¶: {first_frame.shape}")
        
        # ç¡®ä¿å°ºå¯¸è¶³å¤Ÿå¤§
        if height < 256 or width < 256:
            raise ValueError(f"è¾“å…¥å°ºå¯¸å¤ªå°: {width}x{height}ï¼Œè¯·ç¡®ä¿å®½åº¦å’Œé«˜åº¦è‡³å°‘ä¸º256åƒç´ ")
        
        # ç¡®ä¿æˆ‘ä»¬æœ‰ä¸€ä¸ªåŠ è½½å¥½çš„transformer
        if "transformer" not in fp_pipeline or "dtype" not in fp_pipeline:
            raise ValueError("æ— æ•ˆçš„Pipelineå¯¹è±¡ã€‚è¯·ä½¿ç”¨Load FramePack PipelineèŠ‚ç‚¹åŠ è½½æœ‰æ•ˆçš„æ¨¡å‹ã€‚")
        
        transformer = fp_pipeline["transformer"]
        dtype = fp_pipeline["dtype"]
        
        # è®¾å¤‡å’Œæ•°æ®ç±»å‹å‡†å¤‡
        device = model_management.get_torch_device()
        offload_device = model_management.unet_offload_device()
        print(f"[FramePack Sampler] ä½¿ç”¨è®¾å¤‡: {device}, ç²¾åº¦: {dtype}")
        
        # è®¡ç®—æ½œå˜é‡å°ºå¯¸
        latent_height = height // 8
        latent_width = width // 8
        
        # è®¡ç®—è§†é¢‘å¸§æ•°å’Œåˆ†æ®µ
        num_frames_per_window = latent_window_size * 4 - 3  # æ¯ä¸ªçª—å£å¯ç”Ÿæˆçš„æœ‰æ•ˆå¸§æ•°
        total_latent_sections = (total_second_length * fps) / num_frames_per_window
        total_latent_sections = int(max(round(total_latent_sections), 1))
        print(f"[FramePack Sampler] æ€»åˆ†æ®µæ•°: {total_latent_sections}, æ¯æ®µå¸§æ•°: {num_frames_per_window}")
        
        # å†…å­˜ç®¡ç†
        model_management.unload_all_models()
        model_management.cleanup_models()
        model_management.soft_empty_cache()
        
        # å¤„ç†æ¡ä»¶è¾“å…¥
        print("[FramePack Sampler] å¤„ç†æ¡ä»¶è¾“å…¥...")
        
        # å¤„ç†æ­£å‘æ¡ä»¶
        llama_vec = positive[0][0].to(dtype=dtype, device=device)
        clip_l_pooler = positive[0][1]["pooled_output"].to(dtype=dtype, device=device)
        
        # å¤„ç†è´Ÿå‘æ¡ä»¶
        if not math.isclose(cfg, 1.0):  # å¦‚æœéœ€è¦çœŸæ­£çš„CFG
            llama_vec_n = negative[0][0].to(dtype=dtype, device=device)
            clip_l_pooler_n = negative[0][1]["pooled_output"].to(dtype=dtype, device=device)
        else:
            # å¦‚æœCFGä¸º1.0ï¼Œåˆ›å»ºå…¨é›¶æ¡ä»¶
            llama_vec_n = torch.zeros_like(llama_vec, device=device)
            clip_l_pooler_n = torch.zeros_like(clip_l_pooler, device=device)
        
        # è£å‰ªæˆ–å¡«å……LLAMAåµŒå…¥å’Œåˆ›å»ºæ³¨æ„åŠ›æ©ç 
        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)
        llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)
        
        # å‡†å¤‡CLIPè§†è§‰ç‰¹å¾
        image_embeddings = None
        if clip_vision is not None:
            image_embeddings = clip_vision["last_hidden_state"].to(dtype=dtype, device=device)
            print(f"[FramePack Sampler] CLIPè§†è§‰ç‰¹å¾å½¢çŠ¶: {image_embeddings.shape}")
        
        # å‡†å¤‡åˆå§‹æ½œå˜é‡
        batch_size = 1
        initial_latent = None
        
        # ä¼˜åŒ–å¤„ç†start_latentå’Œkeyframesçš„é€»è¾‘
        # å½“åªæœ‰ä¸€ä¸ªkeyframeä¸”æ²¡æœ‰æä¾›start_latentæ—¶ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ç¬¬ä¸€ä¸ªkeyframeä½œä¸ºstart_latent
        if start_latent is None and keyframes is not None and "samples" in keyframes:
            kf_samples = keyframes["samples"]
            if kf_samples.ndim == 5 and kf_samples.shape[2] >= 1:  # è‡³å°‘æœ‰ä¸€ä¸ªå…³é”®å¸§
                print("[FramePack Sampler] æ²¡æœ‰æä¾›start_latentï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®å¸§ä½œä¸ºèµ·å§‹å¸§")
                first_frame = kf_samples[:, :, 0:1, :, :].clone()  # åªå–ç¬¬ä¸€ä¸ªå…³é”®å¸§
                start_latent = {"samples": first_frame}
                print(f"[FramePack Sampler] ä»keyframesæå–èµ·å§‹å¸§ï¼Œå½¢çŠ¶: {first_frame.shape}")
        
        # å¦‚æœæä¾›äº†èµ·å§‹æ½œå˜é‡(I2Væ¨¡å¼)
        if start_latent is not None:
            print("[FramePack Sampler] å‡†å¤‡I2Væ¨¡å¼çš„èµ·å§‹æ½œå˜é‡...")
            # è·å–æ½œå˜é‡å¹¶åº”ç”¨VAEç¼©æ”¾å› å­
            initial_latent = start_latent["samples"] * vae_scaling_factor
            
            # è¯¦ç»†æ‰“å°æ½œå˜é‡çš„å½¢çŠ¶ä¿¡æ¯ç”¨äºè°ƒè¯•
            print(f"[FramePack Sampler] åŸå§‹æ½œå˜é‡å½¢çŠ¶: {initial_latent.shape}, ç±»å‹: {initial_latent.dtype}")
            
            # æ£€æŸ¥æ½œå˜é‡ç»´åº¦æ˜¯å¦æ­£ç¡®
            if initial_latent.ndim == 4:  # å•å¸§æ½œå˜é‡ [B, C, H, W]
                # æ·»åŠ æ—¶é—´ç»´åº¦ [B, C, 1, H, W]
                initial_latent = initial_latent.unsqueeze(2)
                print(f"[FramePack Sampler] æ·»åŠ æ—¶é—´ç»´åº¦åå½¢çŠ¶: {initial_latent.shape}")
            
            # ç¡®è®¤å½¢çŠ¶æ˜¯5ç»´ [B, C, T, H, W]
            if initial_latent.ndim != 5:
                raise ValueError(f"è¾“å…¥æ½œå˜é‡å½¢çŠ¶é”™è¯¯: {initial_latent.shape}ï¼Œåº”ä¸º [B, C, T, H, W] æˆ– [B, C, H, W]")
            
            # å®‰å…¨åœ°è·å–è°ƒæ•´ç›®æ ‡å°ºå¯¸
            current_height = initial_latent.shape[3]
            current_width = initial_latent.shape[4]
            
            # è°ƒæ•´æ½œå˜é‡å°ºå¯¸
            if current_height != latent_height or current_width != latent_width:
                print(f"[FramePack Sampler] è°ƒæ•´æ½œå˜é‡å°ºå¯¸ä» {current_height}x{current_width} åˆ° {latent_height}x{latent_width}")
                
                # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹æ³•é‡å¡‘å’Œè°ƒæ•´å°ºå¯¸
                batch, channels, frames = initial_latent.shape[0], initial_latent.shape[1], initial_latent.shape[2]
                
                # å…ˆå±•å¹³æ‰€æœ‰å¸§è¿›è¡Œå¤„ç†
                flattened = initial_latent.reshape(batch * channels * frames, 1, current_height, current_width)
                print(f"[FramePack Sampler] å±•å¹³åå½¢çŠ¶: {flattened.shape}")
                
                # åº”ç”¨æ’å€¼
                resized = torch.nn.functional.interpolate(
                    flattened, 
                    size=(latent_height, latent_width),
                    mode='bilinear',
                    align_corners=False
                )
                print(f"[FramePack Sampler] è°ƒæ•´å°ºå¯¸åå½¢çŠ¶: {resized.shape}")
                
                # é‡å¡‘å›åŸå§‹ç»´åº¦ç»“æ„
                initial_latent = resized.reshape(batch, channels, frames, latent_height, latent_width)
                print(f"[FramePack Sampler] é‡å¡‘åæœ€ç»ˆå½¢çŠ¶: {initial_latent.shape}")
            
            # å°†æ½œå˜é‡ç§»åŠ¨åˆ°è®¡ç®—è®¾å¤‡
            initial_latent = initial_latent.to(device=device, dtype=dtype)
            print(f"[FramePack Sampler] æœ€ç»ˆèµ·å§‹æ½œå˜é‡å½¢çŠ¶: {initial_latent.shape}")
        
        # åˆå§‹åŒ–èµ·å§‹æ½œå˜é‡(å¦‚æœæ˜¯T2Væ¨¡å¼æˆ–æ²¡æœ‰æä¾›èµ·å§‹æ½œå˜é‡)
        if initial_latent is None:
            start_latent = torch.zeros(
                (batch_size, 16, 1, latent_height, latent_width),
                dtype=torch.float32,
                device="cpu"
            )
        else:
            # å¦‚æœæœ‰èµ·å§‹æ½œå˜é‡ï¼Œç”¨å®ƒåˆå§‹åŒ–start_latent
            start_latent = initial_latent.detach().cpu().to(torch.float32)
            if start_latent.shape[2] > 1:
                # åªå–ç¬¬ä¸€å¸§
                start_latent = start_latent[:, :, :1, :, :]
        
        # åˆå§‹åŒ–å†å²æ½œå˜é‡
        history_latents = torch.zeros(
            (batch_size, 16, 1 + 2 + 16, latent_height, latent_width),
            dtype=torch.float32, 
            device="cpu"
        )
        
        # å‡†å¤‡éšæœºç”Ÿæˆå™¨
        generator = torch.Generator("cpu").manual_seed(seed)
        
        # åˆ›å»ºComfyUIæ¨¡å‹å°è£…
        comfy_model = HyVideoModel(
            HyVideoModelConfig(dtype),
            model_type=comfy.model_base.ModelType.FLOW,
            device=device,
        )
        
        # åˆ›å»ºæ¨¡å‹patcher
        patcher = comfy.model_patcher.ModelPatcher(comfy_model, device, torch.device("cpu"))
        
        # åˆ›å»ºè¿›åº¦æ¡å›è°ƒå‡½æ•° - ä¿®æ”¹ä¸ºæ›´å‡†ç¡®åœ°åæ˜ å¤šåˆ†æ®µè¿›åº¦
        # è®¡ç®—æ€»æ­¥æ•°ä¸º steps * total_latent_sections
        total_steps = steps * total_latent_sections
        progress_bar = comfy.utils.ProgressBar(total_steps)
        
        # è®°å½•å½“å‰åˆ†æ®µç´¢å¼•å’Œå·²å®Œæˆçš„åˆ†æ®µæ•°
        current_section_index = 0
        
        # å®šä¹‰ä¸€ä¸ªé€‚é…k_diffusionåº“è°ƒç”¨æ ¼å¼çš„å›è°ƒå‡½æ•°
        def callback_adapter(d):
            # k_diffusionçš„callbackä¼ å…¥å‚æ•°æ˜¯ä¸€ä¸ªå­—å…¸: {'x': x, 'i': i, 'denoised': model_prev_list[-1]}
            if 'i' in d:
                step = d['i']
                # åªæ›´æ–°ä¸€æ­¥ï¼Œå› ä¸ºæ€»æ­¥æ•°å·²ç»æ˜¯è€ƒè™‘äº†æ‰€æœ‰åˆ†æ®µçš„
                progress_bar.update(1)
                
                # æ‰“å°æ›´è¯¦ç»†çš„è¿›åº¦ä¿¡æ¯
                if step % 5 == 0 or step == steps - 1:  # æ¯5æ­¥æˆ–æœ€åä¸€æ­¥æ‰“å°ä¸€æ¬¡
                    section_progress = f"{current_section_index + 1}/{total_latent_sections}"
                    overall_progress = f"{(current_section_index * steps + step + 1)}/{total_steps}"
                    print(f"[FramePack Sampler] è¿›åº¦: åˆ†æ®µ {section_progress}, æ­¥éª¤ {step + 1}/{steps}, æ€»è¿›åº¦ {overall_progress}")
            return None
            
        # ---------- æ”¹è¿›çš„æ¨¡å‹åŠ è½½ä¸å†…å­˜ç®¡ç† ----------
        print(f"[FramePack Sampler] å¼€å§‹åŠ è½½æ¨¡å‹åˆ°GPUè®¾å¤‡ï¼Œå†…å­˜ä¿ç•™é‡è®¾ç½®ä¸º {gpu_memory_preservation} GB")
        
        # æ£€æŸ¥å¯ç”¨å†…å­˜å¹¶ä¼°ç®—æ¨¡å‹å¤§å°
        try:
            current_free_memory = get_cuda_free_memory_gb(device)
            print(f"[FramePack Sampler] å½“å‰GPUå¯ç”¨å†…å­˜: {current_free_memory:.2f} GB")
            
            # å¦‚æœå†…å­˜ä¿ç•™å€¼å¤§äºå½“å‰å¯ç”¨å†…å­˜çš„80%ï¼Œå‘å‡ºè­¦å‘Šå¹¶è°ƒæ•´
            if gpu_memory_preservation > current_free_memory * 0.8:
                adjusted_preservation = current_free_memory * 0.5  # è°ƒæ•´ä¸ºå¯ç”¨å†…å­˜çš„50%
                print(f"[FramePack Sampler] è­¦å‘Š: å†…å­˜ä¿ç•™å€¼({gpu_memory_preservation}GB)è¿‡å¤§, è‡ªåŠ¨è°ƒæ•´ä¸º {adjusted_preservation:.2f}GB")
                gpu_memory_preservation = adjusted_preservation
            
            # åœ¨åŠ è½½å‰å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜
            if current_free_memory <= gpu_memory_preservation + 1.0:  # éœ€è¦è‡³å°‘ä¿ç•™å€¼+1GB
                print(f"[FramePack Sampler] è­¦å‘Š: GPUå†…å­˜ä¸è¶³! å¯ç”¨: {current_free_memory:.2f}GB, éœ€è¦: >{gpu_memory_preservation+1.0}GB")
                print(f"[FramePack Sampler] å°è¯•é™ä½é‡‡æ ·åˆ†è¾¨ç‡æˆ–é™ä½ä¿ç•™å†…å­˜å€¼")
        except Exception as e:
            print(f"[FramePack Sampler] å†…å­˜æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}")
            print("[FramePack Sampler] ç»§ç»­æ‰§è¡Œï¼Œä½†å¯èƒ½ä¸ç¨³å®š")
        
        # æ”¹è¿›çš„æ¨¡å‹åŠ è½½æ–¹æ³•
        try:
            # åˆ†é˜¶æ®µåŠ è½½æ¨¡å‹ï¼Œæ¯é˜¶æ®µæ£€æŸ¥å†…å­˜
            print(f"[FramePack Sampler] é˜¶æ®µ1: å°†æ¨¡å‹ç§»åŠ¨è‡³ {device}...")
            move_model_to_device_with_memory_preservation(
                transformer, 
                target_device=device, 
                preserved_memory_gb=gpu_memory_preservation
            )
            
            # æ£€æŸ¥åŠ è½½åçš„å†…å­˜çŠ¶æ€
            try:
                post_load_memory = get_cuda_free_memory_gb(device)
                print(f"[FramePack Sampler] æ¨¡å‹åŠ è½½åGPUå¯ç”¨å†…å­˜: {post_load_memory:.2f} GB")
                
                if post_load_memory < gpu_memory_preservation:
                    print(f"[FramePack Sampler] æ³¨æ„: åŠ è½½åå¯ç”¨å†…å­˜({post_load_memory:.2f}GB)ä½äºä¿ç•™ç›®æ ‡({gpu_memory_preservation}GB)")
                    print(f"[FramePack Sampler] å°†å°è¯•ç»§ç»­è¿è¡Œï¼Œä½†å¯èƒ½ä¼šå‡ºç°å†…å­˜ä¸è¶³é”™è¯¯")
            except Exception as e:
                print(f"[FramePack Sampler] åŠ è½½åå†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
        
        except Exception as e:
            print(f"[FramePack Sampler] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(f"[FramePack Sampler] å°è¯•ä½¿ç”¨å¤‡ç”¨åŠ è½½æ–¹æ³•...")
            
            # å¤‡ç”¨åŠ è½½æ–¹æ³• - ç›´æ¥åŠ è½½ä½†ä¸ç®¡ç†å†…å­˜
            transformer.to(device)
            print("[FramePack Sampler] ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åŠ è½½æ¨¡å‹å®Œæˆ")
        
        # è¿è¡Œé‡‡æ ·
        print("[FramePack Sampler] å¼€å§‹é‡‡æ ·...")
        print(f"  - å°ºå¯¸: {width}x{height}, æ€»å¸§æ•°: {total_second_length * fps}")
        print(f"  - åˆ†æ®µæ•°: {total_latent_sections}, æ¯æ®µçª—å£å¤§å°: {latent_window_size}")
        print(f"  - æ­¥æ•°: {steps}, CFG: {cfg}, Guidance Scale: {guidance_scale}")
        print(f"  - ç§å­: {seed}, ç§»ä½: {shift}")
        if initial_latent is not None:
            print(f"  - I2Væ¨¡å¼: å»å™ªå¼ºåº¦ = {denoise_strength}")
        
        try:
            # å¤„ç†åˆ†æ®µç”Ÿæˆ
            total_generated_latent_frames = 0
            latent_paddings_list = list(reversed(range(total_latent_sections)))
            latent_paddings = latent_paddings_list.copy()
            
            # å¯¹äºé•¿è§†é¢‘ï¼Œä¼˜åŒ–åˆ†æ®µç­–ç•¥
            if total_latent_sections > 4:
                latent_paddings = [3] + [2] * (total_latent_sections - 3) + [1, 0]
                latent_paddings_list = latent_paddings.copy()
            
            # å¤„ç†å…³é”®å¸§ç´¢å¼• - å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
            keyframe_idx_list = []
            if keyframes is not None and keyframe_indices:
                try:
                    # è§£æç´¢å¼•å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "0,5,10"
                    keyframe_idx_list = [int(idx.strip()) for idx in keyframe_indices.split(',') if idx.strip()]
                    
                    # éªŒè¯ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
                    if not keyframe_idx_list:
                        print("[FramePack Sampler] è­¦å‘Š: æä¾›äº†keyframesä½†keyframe_indicesä¸ºç©ºï¼Œå…³é”®å¸§åŠŸèƒ½å°†è¢«ç¦ç”¨")
                    else:
                        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦ä¸ºå‡åº
                        if sorted(keyframe_idx_list) != keyframe_idx_list:
                            print("[FramePack Sampler] è­¦å‘Š: keyframe_indicesä¸æ˜¯å‡åºï¼Œè‡ªåŠ¨æ’åº")
                            keyframe_idx_list = sorted(keyframe_idx_list)
                        
                        # æ£€æŸ¥å…³é”®å¸§æ•°é‡æ˜¯å¦ä¸ç´¢å¼•åŒ¹é…
                        if keyframes is not None and "samples" in keyframes:
                            kf_samples = keyframes["samples"]
                            if kf_samples.ndim == 5:  # [B, C, T, H, W]
                                num_keyframes = kf_samples.shape[2]
                                if num_keyframes != len(keyframe_idx_list):
                                    print(f"[FramePack Sampler] è­¦å‘Š: keyframe_indicesé•¿åº¦({len(keyframe_idx_list)})ä¸keyframesæ•°é‡({num_keyframes})ä¸åŒ¹é…")
                                    if num_keyframes < len(keyframe_idx_list):
                                        # æˆªæ–­ç´¢å¼•åˆ—è¡¨ä»¥åŒ¹é…å…³é”®å¸§æ•°é‡
                                        keyframe_idx_list = keyframe_idx_list[:num_keyframes]
                                        print(f"[FramePack Sampler] ç´¢å¼•åˆ—è¡¨å·²æˆªæ–­ä¸º: {keyframe_idx_list}")
                            else:
                                print(f"[FramePack Sampler] è­¦å‘Š: keyframesç»´åº¦ä¸æ­£ç¡®: {kf_samples.shape}, é¢„æœŸä¸º[B, C, T, H, W]")
                        
                        print(f"[FramePack Sampler] ä½¿ç”¨å…³é”®å¸§ç´¢å¼•: {keyframe_idx_list}, å¼•å¯¼å¼ºåº¦: {keyframe_guidance_strength}")
                except Exception as e:
                    print(f"[FramePack Sampler] è§£ækeyframe_indiceså‡ºé”™: {e}")
                    print(f"[FramePack Sampler] è¯·ç¡®ä¿æ ¼å¼æ­£ç¡®ï¼Œä¾‹å¦‚: '0,5,10'")
                    keyframe_idx_list = []  # é‡ç½®ä¸ºç©ºåˆ—è¡¨
            
            # é€æ®µç”Ÿæˆ
            for latent_padding in latent_paddings:
                print(f"[FramePack Sampler] ç”Ÿæˆåˆ†æ®µ {latent_padding + 1}/{total_latent_sections}")
                is_last_section = latent_padding == 0
                latent_padding_size = latent_padding * latent_window_size
                
                print(f'  - å¡«å……å¤§å° = {latent_padding_size}, æ˜¯å¦æœ€ååˆ†æ®µ = {is_last_section}')
                
                # åˆ›å»ºå’Œåˆ†å‰²ç´¢å¼•
                indices = torch.arange(0, sum([1, latent_padding_size, latent_window_size, 1, 2, 16])).unsqueeze(0)
                clean_latent_indices_pre, blank_indices, latent_indices, clean_latent_indices_post, clean_latent_2x_indices, clean_latent_4x_indices = indices.split(
                    [1, latent_padding_size, latent_window_size, 1, 2, 16], dim=1
                )
                clean_latent_indices = torch.cat([clean_latent_indices_pre, clean_latent_indices_post], dim=1)
                
                # è®¡ç®—å½“å‰åˆ†æ®µç´¢å¼•
                current_section_index = total_latent_sections - latent_padding - 1
                
                # --- å¼€å§‹: å…³é”®å¸§å¤„ç†é€»è¾‘ ---
                current_base_latent = start_latent.to(history_latents)  # é»˜è®¤ä½¿ç”¨start_latent
                calculated_weight = 1.0  # é»˜è®¤æƒé‡
                
                if keyframes is not None and len(keyframe_idx_list) > 0:
                    # è®¡ç®—å½“å‰ä½äºè§†é¢‘çš„å“ªä¸ªåˆ†æ®µ
                    total_sections = total_latent_sections  # æ€»åˆ†æ®µæ•°
                    forward_section_no = current_section_index  # ä»å‰åˆ°åè®¡ç®—çš„åˆ†æ®µç´¢å¼•
                    
                    print(f"[å…³é”®å¸§é€»è¾‘] å½“å‰å¤„ç†åˆ†æ®µ{forward_section_no}/{total_sections-1}, å…±æœ‰{len(keyframe_idx_list)}ä¸ªå…³é”®å¸§")
                    
                    # è·å–å…³é”®å¸§æ½œå˜é‡å¹¶åº”ç”¨VAEç¼©æ”¾å› å­
                    kf_samples = keyframes["samples"] * vae_scaling_factor  # åº”ç”¨VAEç¼©æ”¾å› å­
                    kf_samples = kf_samples.to(dtype=dtype, device="cpu")   # ç¡®ä¿ç±»å‹å’Œè®¾å¤‡ä¸€è‡´
                    
                    # è®°å½•å½“å‰å…³é”®å¸§ç´¢å¼•å’Œä¸‹ä¸€ä¸ªå…³é”®å¸§ç´¢å¼•
                    idx_current = None
                    next_idx = None
                    
                    # å¤„ç†ä¸åŒæƒ…å†µ
                    if forward_section_no < keyframe_idx_list[0]:
                        # æƒ…å†µ1: åœ¨ç¬¬ä¸€ä¸ªå…³é”®å¸§ä¹‹å‰
                        # ä½¿ç”¨start_latentï¼Œå› ä¸ºè¿™æ—¶è¿˜æ²¡æœ‰åˆ°ç¬¬ä¸€ä¸ªå…³é”®å¸§
                        current_base_latent = start_latent.to(history_latents)
                        idx_current = 0  # è§†é¢‘å¼€å§‹ä½ç½®
                        next_idx = keyframe_idx_list[0]  # ç¬¬ä¸€ä¸ªå…³é”®å¸§ä½ç½®
                        
                        # è®¡ç®—å‘ç¬¬ä¸€ä¸ªå…³é”®å¸§è¿‡æ¸¡çš„è¿›åº¦
                        distance = next_idx - idx_current
                        progress = 0.0
                        if distance > 0:
                            progress = forward_section_no / distance  # ä»0åˆ°1çš„è¿›åº¦
                            progress = max(0.0, min(1.0, progress))  # ç¡®ä¿åœ¨0åˆ°1ä¹‹é—´
                        
                        # æ ¹æ®è·ç¦»ç¬¬ä¸€ä¸ªå…³é”®å¸§çš„è¿œè¿‘åº”ç”¨ä¸åŒæƒé‡
                        calculated_weight = 1.0 + (keyframe_guidance_strength - 1.0) * progress * 0.5
                        print(f"[å…³é”®å¸§é€»è¾‘] åˆ†æ®µ{forward_section_no}: åœ¨ç¬¬ä¸€ä¸ªå…³é”®å¸§å‰ï¼Œä½¿ç”¨èµ·å§‹å¸§ï¼Œè¿›åº¦={progress:.2f}ï¼Œæƒé‡={calculated_weight:.2f}")
                    
                    elif forward_section_no >= keyframe_idx_list[-1]:
                        # æƒ…å†µ2: åœ¨æœ€åä¸€ä¸ªå…³é”®å¸§ä¹‹å
                        # ç›´æ¥ä½¿ç”¨æœ€åä¸€ä¸ªå…³é”®å¸§
                        last_kf_idx = len(keyframe_idx_list) - 1
                        current_base_latent = kf_samples[:, :, last_kf_idx:last_kf_idx+1, :, :].to(history_latents)
                        idx_current = keyframe_idx_list[-1]
                        
                        # æ ¹æ®è·ç¦»æœ€åä¸€ä¸ªå…³é”®å¸§çš„è¿œè¿‘è®¡ç®—æƒé‡ï¼ˆè¶Šè¿œæƒé‡è¶Šä½ï¼‰
                        distance_from_last = forward_section_no - idx_current
                        max_distance = total_sections - idx_current
                        decay_factor = 1.0
                        if max_distance > 0:
                            decay_factor = 1.0 - min(1.0, distance_from_last / max_distance)
                        
                        calculated_weight = 1.0 + (keyframe_guidance_strength - 1.0) * decay_factor
                        print(f"[å…³é”®å¸§é€»è¾‘] åˆ†æ®µ{forward_section_no}: åœ¨æœ€åä¸€ä¸ªå…³é”®å¸§{idx_current}ä¹‹åï¼Œæƒé‡={calculated_weight:.2f}")
                    
                    elif forward_section_no in keyframe_idx_list:
                        # æƒ…å†µ3: å½“å‰åˆ†æ®µæ°å¥½æ˜¯æŸä¸ªå…³é”®å¸§ä½ç½®
                        kf_pos = keyframe_idx_list.index(forward_section_no)
                        current_base_latent = kf_samples[:, :, kf_pos:kf_pos+1, :, :].to(history_latents)
                        # åœ¨å…³é”®å¸§ä½ç½®ä½¿ç”¨å®Œæ•´æƒé‡
                        calculated_weight = keyframe_guidance_strength
                        print(f"[å…³é”®å¸§é€»è¾‘] åˆ†æ®µ{forward_section_no}: æ°å¥½æ˜¯å…³é”®å¸§ä½ç½®ï¼Œä½¿ç”¨å®Œæ•´æƒé‡{calculated_weight:.2f}")
                    
                    else:
                        # æƒ…å†µ4: åœ¨ä¸¤ä¸ªå…³é”®å¸§ä¹‹é—´
                        # æ‰¾åˆ°å½“å‰åˆ†æ®µæ‰€åœ¨çš„ä¸¤ä¸ªå…³é”®å¸§ä¹‹é—´
                        for i in range(1, len(keyframe_idx_list)):
                            if keyframe_idx_list[i-1] <= forward_section_no < keyframe_idx_list[i]:
                                prev_kf_idx = i-1
                                next_kf_idx = i
                                idx_current = keyframe_idx_list[prev_kf_idx]
                                next_idx = keyframe_idx_list[next_kf_idx]
                                
                                # è®¡ç®—åœ¨ä¸¤ä¸ªå…³é”®å¸§ä¹‹é—´çš„ä½ç½®
                                segment_width = next_idx - idx_current
                                if segment_width > 0:
                                    # è®¡ç®—å½“å‰ä½ç½®åœ¨åŒºé—´å†…çš„è¿›åº¦ (0=å‰ä¸€ä¸ªå…³é”®å¸§ï¼Œ1=ä¸‹ä¸€ä¸ªå…³é”®å¸§)
                                    progress = (forward_section_no - idx_current) / segment_width
                                    progress = max(0.0, min(1.0, progress))  # ç¡®ä¿åœ¨0åˆ°1ä¹‹é—´
                                    
                                    # æ ¹æ®è¿›åº¦é€‰æ‹©ä½¿ç”¨å“ªä¸ªå…³é”®å¸§
                                    if progress < 0.5:
                                        # æ›´æ¥è¿‘å‰ä¸€ä¸ªå…³é”®å¸§ï¼Œä½¿ç”¨å‰ä¸€ä¸ª
                                        current_base_latent = kf_samples[:, :, prev_kf_idx:prev_kf_idx+1, :, :].to(history_latents)
                                        # è¶Šæ¥è¿‘å…³é”®å¸§ï¼Œæƒé‡è¶Šå¤§
                                        influence = 1.0 - progress * 2  # ä»1.0åˆ°0.0
                                        calculated_weight = 1.0 + (keyframe_guidance_strength - 1.0) * influence
                                        print(f"[å…³é”®å¸§é€»è¾‘] åˆ†æ®µ{forward_section_no}: æ›´æ¥è¿‘å‰ä¸€ä¸ªå…³é”®å¸§{idx_current}ï¼Œè¿›åº¦={progress:.2f}ï¼Œæƒé‡={calculated_weight:.2f}")
                                    else:
                                        # æ›´æ¥è¿‘ä¸‹ä¸€ä¸ªå…³é”®å¸§ï¼Œä½¿ç”¨ä¸‹ä¸€ä¸ª
                                        current_base_latent = kf_samples[:, :, next_kf_idx:next_kf_idx+1, :, :].to(history_latents)
                                        # è¶Šæ¥è¿‘å…³é”®å¸§ï¼Œæƒé‡è¶Šå¤§
                                        influence = (progress - 0.5) * 2  # ä»0.0åˆ°1.0
                                        calculated_weight = 1.0 + (keyframe_guidance_strength - 1.0) * influence
                                        print(f"[å…³é”®å¸§é€»è¾‘] åˆ†æ®µ{forward_section_no}: æ›´æ¥è¿‘ä¸‹ä¸€ä¸ªå…³é”®å¸§{next_idx}ï¼Œè¿›åº¦={progress:.2f}ï¼Œæƒé‡={calculated_weight:.2f}")
                                else:
                                    # ä¸¤ä¸ªå…³é”®å¸§ç´¢å¼•ç›¸åŒçš„å¼‚å¸¸æƒ…å†µï¼Œä½¿ç”¨å‰ä¸€ä¸ª
                                    current_base_latent = kf_samples[:, :, prev_kf_idx:prev_kf_idx+1, :, :].to(history_latents)
                                    calculated_weight = keyframe_guidance_strength
                                    print(f"[å…³é”®å¸§é€»è¾‘] åˆ†æ®µ{forward_section_no}: å…³é”®å¸§{idx_current}å’Œ{next_idx}ä½äºåŒä¸€ä½ç½®ï¼Œä½¿ç”¨å‰ä¸€ä¸ªå…³é”®å¸§")
                                break
                else:
                    print(f"[å…³é”®å¸§é€»è¾‘] åˆ†æ®µ{current_section_index}: æœªæä¾›å…³é”®å¸§æˆ–å…³é”®å¸§ç´¢å¼•ä¸ºç©ºï¼Œä½¿ç”¨start_latent")
                
                # åº”ç”¨æƒé‡åˆ°é€‰æ‹©çš„åŸºç¡€æ½œå˜é‡
                clean_latents_pre = current_base_latent * calculated_weight
                # --- ç»“æŸ: å…³é”®å¸§å¤„ç†é€»è¾‘ ---
                
                # åŸå§‹é€»è¾‘ï¼Œå¤„ç†clean_latents_post, _2x, _4x
                clean_latents_post, clean_latents_2x, clean_latents_4x = history_latents[:, :, :1 + 2 + 16, :, :].split([1, 2, 16], dim=2)
                clean_latents = torch.cat([clean_latents_pre, clean_latents_post], dim=2)
                
                # è®¾ç½®teacache
                if hasattr(transformer, 'initialize_teacache'):
                    if use_teacache:
                        transformer.initialize_teacache(enable_teacache=True, num_steps=steps, rel_l1_thresh=teacache_thresh)
                    else:
                        transformer.initialize_teacache(enable_teacache=False)
                
                # æ£€æŸ¥å½“å‰å†…å­˜çŠ¶æ€
                try:
                    current_mem = get_cuda_free_memory_gb(device)
                    print(f"[FramePack Sampler] é‡‡æ ·å‰GPUå¯ç”¨å†…å­˜: {current_mem:.2f} GB")
                    if current_mem < 1.0:  # å¦‚æœå†…å­˜ä¸¥é‡ä¸è¶³
                        print("[FramePack Sampler] è­¦å‘Š: GPUå†…å­˜ä¸¥é‡ä¸è¶³ï¼Œå°è¯•é‡Šæ”¾éƒ¨åˆ†å†…å­˜...")
                        torch.cuda.empty_cache()
                except Exception as e:
                    print(f"[FramePack Sampler] å†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
                
                # æ‰§è¡Œé‡‡æ ·
                with torch.autocast(device_type=model_management.get_autocast_device(device), dtype=dtype, enabled=True):
                    # æ›´æ–°å½“å‰åˆ†æ®µç´¢å¼•
                    print(f"[FramePack Sampler] å¼€å§‹å¤„ç†åˆ†æ®µ {current_section_index + 1}/{total_latent_sections}")
                    
                    generated_latents = sample_hunyuan(
                        transformer=transformer,
                        sampler=sampler,  # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„é‡‡æ ·å™¨
                        initial_latent=initial_latent,  # ä½¿ç”¨åˆå§‹æ½œå˜é‡(I2Væ¨¡å¼)
                        concat_latent=None,
                        strength=denoise_strength,  # åº”ç”¨å»å™ªå¼ºåº¦
                        width=width,
                        height=height,
                        frames=num_frames_per_window,
                        real_guidance_scale=cfg,
                        distilled_guidance_scale=guidance_scale,
                        guidance_rescale=0.0,
                        shift=shift if shift > 0 else None,
                        num_inference_steps=steps,
                        batch_size=batch_size,
                        generator=generator,
                        prompt_embeds=llama_vec,
                        prompt_embeds_mask=llama_attention_mask,
                        prompt_poolers=clip_l_pooler,
                        negative_prompt_embeds=llama_vec_n,
                        negative_prompt_embeds_mask=llama_attention_mask_n,
                        negative_prompt_poolers=clip_l_pooler_n,
                        dtype=dtype,
                        device=device,
                        negative_kwargs=None,
                        callback=callback_adapter,  # ä½¿ç”¨æˆ‘ä»¬çš„é€‚é…å™¨å›è°ƒå‡½æ•°
                        # æ·»åŠ é¢å¤–å‚æ•°
                        image_embeddings=image_embeddings,
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                    )
                
                # æ£€æŸ¥é‡‡æ ·åçš„å†…å­˜çŠ¶æ€
                try:
                    post_sample_mem = get_cuda_free_memory_gb(device)
                    print(f"[FramePack Sampler] åˆ†æ®µé‡‡æ ·åGPUå¯ç”¨å†…å­˜: {post_sample_mem:.2f} GB")
                    
                    # å¦‚æœå†…å­˜ä½äºä¿ç•™å€¼çš„50%ï¼Œæ¸…ç†ç¼“å­˜
                    if post_sample_mem < gpu_memory_preservation * 0.5:
                        print("[FramePack Sampler] å†…å­˜ä¸è¶³ï¼Œæ¸…ç†ç¼“å­˜...")
                        torch.cuda.empty_cache()
                except Exception as e:
                    print(f"[FramePack Sampler] é‡‡æ ·åå†…å­˜æ£€æŸ¥å‡ºé”™: {e}")
                
                # å¦‚æœæ˜¯æœ€åä¸€æ®µï¼Œè¿æ¥èµ·å§‹æ½œå˜é‡
                if is_last_section:
                    generated_latents = torch.cat([start_latent.to(generated_latents), generated_latents], dim=2)
                
                # æ›´æ–°æ€»å¸§æ•°å’Œå†å²æ½œå˜é‡
                total_generated_latent_frames += int(generated_latents.shape[2])
                history_latents = torch.cat([generated_latents.to(history_latents), history_latents], dim=2)
                
                # è·å–å®é™…ç”Ÿæˆçš„æ½œå˜é‡
                real_history_latents = history_latents[:, :, :total_generated_latent_frames, :, :]
                
                # å¦‚æœæ˜¯æœ€åä¸€æ®µï¼Œåœæ­¢ç”Ÿæˆ
                if is_last_section:
                    break
            
            print("[FramePack Sampler] é‡‡æ ·å®Œæˆ.")
            print(f"[FramePack Sampler] è¾“å‡ºå½¢çŠ¶: {real_history_latents.shape}")
            
            # è®°å½•å®é™…è°ƒç”¨çš„é‡‡æ ·ä¿¡æ¯
            print(f"[FramePack Sampler] ğŸ“Š é‡‡æ ·ä¿¡æ¯: çª—å£å¤§å°={latent_window_size}, å¸§æ•°/çª—å£={num_frames_per_window}")
            
            # ç¡®ä¿è¿›åº¦æ¡æ˜¾ç¤ºä¸ºå®ŒæˆçŠ¶æ€
            progress_remaining = total_steps - (current_section_index + 1) * steps
            if progress_remaining > 0:
                print(f"[FramePack Sampler] æ›´æ–°è¿›åº¦æ¡åˆ°å®ŒæˆçŠ¶æ€ (å‰©ä½™ {progress_remaining} æ­¥)")
                progress_bar.update(progress_remaining)
            print("[FramePack Sampler] âœ… è¿›åº¦: 100% å®Œæˆ!")
            
            # è¿”å›ç»“æœï¼Œåº”ç”¨VAEç¼©æ”¾å› å­
            return ({"samples": real_history_latents.to(model_management.intermediate_device()) / vae_scaling_factor},)
            
        except Exception as e:
            # è¯¦ç»†è®°å½•é”™è¯¯ä¿¡æ¯
            error_message = f"[FramePack Sampler] âŒ é”™è¯¯: {str(e)}"
            print(error_message)
            print("[FramePack Sampler] ğŸ“‹ é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            
            # æ›´æ–°è¿›åº¦æ¡åˆ°é”™è¯¯çŠ¶æ€
            try:
                # è®¡ç®—å‰©ä½™æ­¥æ•°å¹¶æ›´æ–°è¿›åº¦æ¡
                if 'progress_bar' in locals() and 'current_section_index' in locals() and 'total_steps' in locals():
                    progress_so_far = min(current_section_index * steps, total_steps)
                    progress_remaining = total_steps - progress_so_far
                    if progress_remaining > 0:
                        print(f"[FramePack Sampler] ç”±äºé”™è¯¯æ›´æ–°è¿›åº¦æ¡ (å‰©ä½™ {progress_remaining} æ­¥)")
                        progress_bar.update(progress_remaining)
                print("[FramePack Sampler] âš ï¸ è¿›åº¦: ç”±äºé”™è¯¯è€Œä¸­æ–­!")
            except Exception as progress_error:
                print(f"[FramePack Sampler] æ›´æ–°è¿›åº¦æ¡å¤±è´¥: {progress_error}")
            
            # æä¾›é€šç”¨çš„å»ºè®®è§£å†³æ–¹æ¡ˆ
            print("[FramePack Sampler] ğŸ”§ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            print("1. æ£€æŸ¥GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿï¼Œå¯èƒ½éœ€è¦é™ä½åˆ†è¾¨ç‡æˆ–å‡å°‘ç”Ÿæˆçš„å¸§æ•°")
            print("2. ç¡®ä¿æ¨¡å‹æ­£ç¡®åŠ è½½")
            print("3. æ£€æŸ¥æ¡ä»¶è¾“å…¥æ˜¯å¦æœ‰æ•ˆ")
            print("4. å¦‚æœé—®é¢˜æŒç»­ï¼Œå¯ä»¥å°è¯•é‡å¯ComfyUIæˆ–æ¸…ç†ç¼“å­˜")
            
            # åˆ›å»ºä¸€ä¸ªç©ºçš„æœ‰æ•ˆæ½œå˜é‡ä½œä¸ºè¿”å›å€¼
            try:
                print("[FramePack Sampler] åˆ›å»ºç©ºæ½œå˜é‡ä½œä¸ºé”™è¯¯æ¢å¤...")
                # åˆ›å»ºä¸€ä¸ªå°çš„ç©ºæ½œå˜é‡
                empty_latent = torch.zeros(
                    (1, 16, 1, height // 8, width // 8), 
                    dtype=torch.float32,
                    device=model_management.intermediate_device()
                )
                return ({"samples": empty_latent},)
            except Exception as fallback_error:
                print(f"[FramePack Sampler] åˆ›å»ºç©ºæ½œå˜é‡å¤±è´¥: {fallback_error}")
                # æœ€å°çš„å¯èƒ½æ½œå˜é‡
                minimal_latent = torch.zeros(
                    (1, 4, 1, 8, 8), 
                    dtype=torch.float32,
                    device=model_management.intermediate_device()
                )
                return ({"samples": minimal_latent},)
        finally:
            # ä¸»åŠ¨é‡Šæ”¾å†…å­˜
            print("[FramePack Sampler] ä¸»åŠ¨æ¸…ç†GPUå†…å­˜...")
            torch.cuda.empty_cache()
            
            # é‡Šæ”¾transformer
            try:
                print(f"[FramePack Sampler] å°†transformerå¸è½½åˆ° {offload_device}")
                transformer.to(offload_device)
            except Exception as e:
                print(f"[FramePack Sampler] å¸è½½transformeræ—¶å‡ºé”™: {e}")
            
            model_management.soft_empty_cache()


NODE_CLASS_MAPPINGS = {
    "FramePackDiffusersSampler_HY": FramePackDiffusersSampler
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "FramePackDiffusersSampler_HY": "FramePack Sampler (HY)"
} 

# å¯¼å…¥è¾…åŠ©èŠ‚ç‚¹å¹¶æ·»åŠ åˆ°æ˜ å°„ä¸­
try:
    from .keyframe_helper import NODE_CLASS_MAPPINGS as KEYFRAME_NODE_CLASS_MAPPINGS
    from .keyframe_helper import NODE_DISPLAY_NAME_MAPPINGS as KEYFRAME_NODE_DISPLAY_NAME_MAPPINGS
    
    # æ›´æ–°æ˜ å°„
    NODE_CLASS_MAPPINGS.update(KEYFRAME_NODE_CLASS_MAPPINGS)
    NODE_DISPLAY_NAME_MAPPINGS.update(KEYFRAME_NODE_DISPLAY_NAME_MAPPINGS)
except ImportError as e:
    print(f"[FramePack] è­¦å‘Š: æ— æ³•å¯¼å…¥å…³é”®å¸§è¾…åŠ©èŠ‚ç‚¹: {e}") 